# spider

#### ä»€ä¹ˆæ˜¯äº’è”ç½‘çˆ¬è™«ï¼Ÿ

```
1.é€šè¿‡ä¸€ä¸ªç¨‹åºï¼Œæ ¹æ®Urlè¿›è¡Œçˆ¬å–ç½‘é¡µï¼Œè·å–æœ‰ç”¨ä¿¡æ¯
2.ä½¿ç”¨ç¨‹åºæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œå»å‘æœåŠ¡å™¨å‘é€è¯·æ±‚ï¼Œè·å–å“åº”ä¿¡æ¯
```

#### çˆ¬è™«æ ¸å¿ƒ?

```
1.çˆ¬å–ç½‘é¡µï¼šçˆ¬å–æ•´ä¸ªç½‘é¡µ  åŒ…å«äº†ç½‘é¡µä¸­æ‰€æœ‰å¾—å†…å®¹
2.è§£ææ•°æ®ï¼šå°†ç½‘é¡µä¸­ä½ å¾—åˆ°çš„æ•°æ® è¿›è¡Œè§£æ
3.éš¾ç‚¹ï¼šçˆ¬è™«å’Œåçˆ¬è™«ä¹‹é—´çš„åšå¼ˆ
```

#### çˆ¬è™«çš„ç”¨é€”ï¼Ÿ

- æ•°æ®åˆ†æ/äººå·¥æ•°æ®é›†
- ç¤¾äº¤è½¯ä»¶å†·å¯åŠ¨
- èˆ†æƒ…ç›‘æ§
- ç«äº‰å¯¹æ‰‹ç›‘æ§

#### çˆ¬è™«è¯­è¨€åˆ†ç±»ï¼Ÿ

```
1.php:å¤šè¿›ç¨‹å’Œå¤šçº¿ç¨‹æ”¯æŒä¸å¥½
2.java:ç›®å‰javaçˆ¬è™«éœ€æ±‚å²—ä½æ—ºç››ï¼Œpythonçˆ¬è™«çš„ä¸»è¦å¯¹æ‰‹ï¼Œä»£ç è‡ƒè‚¿ï¼Œä»£ç é‡å¤§ã€é‡æ„æˆæœ¬é«˜ï¼Œè€Œçˆ¬è™«éœ€è¦ç»å¸¸ä¿®æ”¹ï¼Œæ‰€ä»¥ä¸å¥½ç”¨
3.C\C++:å­¦ä¹ æˆæœ¬æ¯”è¾ƒé«˜ï¼Œæ€§èƒ½å’Œæ•ˆç‡é«˜ï¼Œåœç•™åœ¨ç ”ç©¶å±‚é¢ï¼Œå¸‚åœºéœ€æ±‚é‡å°ã€‚ä½“ç°ç¨‹åºå‘˜èƒ½åŠ›ã€‚
4.python:è¯­æ³•ç®€æ´ä¼˜ç¾ã€å¯¹æ–°æ‰‹å‹å¥½ï¼Œå­¦ä¹ æˆæœ¬ä½ã€æ”¯æŒçš„æ¨¡å—éå¸¸å¤šã€æœ‰scrapyéå¸¸å¼ºå¤§çš„çˆ¬è™«æ¡†æ¶
```

#### çˆ¬è™«åˆ†ç±»ï¼Ÿ

```
é€šç”¨çˆ¬è™«ï¼š
	å®ä¾‹
		ç™¾åº¦ã€360ã€googleã€sougouç­‰æœç´¢å¼•æ“---ä¼¯ä¹åœ¨çº¿
	åŠŸèƒ½
		è®¿é—®ç½‘é¡µ->æŠ“å–æ•°æ®->æ•°æ®å­˜å‚¨->æ•°æ®å¤„ç†->æä¾›æ£€ç´¢æœåŠ¡
	robotsåè®®
		ä¸€ä¸ªçº¦å®šä¿—æˆçš„åè®®ï¼Œæ·»åŠ robots.txtæ–‡ä»¶ï¼Œæ¥è¯´æ˜æœ¬ç½‘ç«™å“ªäº›å†…å®¹ä¸å¯ä»¥è¢«æŠ“å–ï¼Œèµ·ä¸åˆ°é™åˆ¶ä½œç”¨
		è‡ªå·±å†™çš„çˆ¬è™«æ— éœ€éµå®ˆ
	ç½‘ç«™æ’å(SEO)
		1. æ ¹æ®pagerankç®—æ³•å€¼è¿›è¡Œæ’åï¼ˆå‚è€ƒä¸ªç½‘ç«™æµé‡ã€ç‚¹å‡»ç‡ç­‰æŒ‡æ ‡ï¼‰
		2. ç™¾åº¦ç«ä»·æ’åï¼Œé’±å¤šå°±æ˜¯çˆ¸çˆ¸
	ç¼ºç‚¹
		1. æŠ“å–çš„æ•°æ®å¤§å¤šæ˜¯æ— ç”¨çš„
		2.ä¸èƒ½æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚æ¥ç²¾å‡†è·å–æ•°æ®
```

```
èšç„¦çˆ¬è™«
	åŠŸèƒ½
		æ ¹æ®éœ€æ±‚ï¼Œå®ç°çˆ¬è™«ç¨‹åºï¼ŒæŠ“å–éœ€è¦çš„æ•°æ®
	åŸç†
		1.ç½‘é¡µéƒ½æœ‰è‡ªå·±å”¯ä¸€çš„url(ç»Ÿä¸€èµ„æºå®šä½ç¬¦ï¼‰
		2.ç½‘é¡µéƒ½æ˜¯htmlç»„æˆ
		3.ä¼ è¾“åè®®éƒ½æ˜¯http\https
	è®¾è®¡æ€è·¯
		1.ç¡®å®šè¦çˆ¬å–çš„url
			å¦‚ä½•è·å–Url
		2.æ¨¡æ‹Ÿæµè§ˆå™¨é€šè¿‡httpåè®®è®¿é—®urlï¼Œè·å–æœåŠ¡å™¨è¿”å›çš„htmlä»£ç 
			å¦‚ä½•è®¿é—®
		3.è§£æhtmlå­—ç¬¦ä¸²ï¼ˆæ ¹æ®ä¸€å®šè§„åˆ™æå–éœ€è¦çš„æ•°æ®ï¼‰
			å¦‚ä½•è§£æ
```

#### åçˆ¬æ‰‹æ®µï¼Ÿ

```
1.User-Agentï¼š
	User Agentä¸­æ–‡åä¸ºç”¨æˆ·ä»£ç†ï¼Œç®€ç§° UAï¼Œå®ƒæ˜¯ä¸€ä¸ªç‰¹æ®Šå­—ç¬¦ä¸²å¤´ï¼Œä½¿å¾—æœåŠ¡å™¨èƒ½å¤Ÿè¯†åˆ«å®¢æˆ·ä½¿ç”¨çš„æ“ä½œç³»ç»ŸåŠç‰ˆæœ¬ã€CPU ç±»å‹ã€æµè§ˆå™¨åŠç‰ˆæœ¬ã€æµè§ˆå™¨æ¸²æŸ“å¼•æ“ã€æµè§ˆå™¨è¯­è¨€ã€æµè§ˆå™¨æ’ä»¶ç­‰ã€‚
2.ä»£ç†IP
	è¥¿æ¬¡ä»£ç†
	å¿«ä»£ç†
	ä»€ä¹ˆæ˜¯é«˜åŒ¿åã€åŒ¿åå’Œé€æ˜ä»£ç†ï¼Ÿå®ƒä»¬æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
		1.ä½¿ç”¨é€æ˜ä»£ç†ï¼Œå¯¹æ–¹æœåŠ¡å™¨å¯ä»¥çŸ¥é“ä½ ä½¿ç”¨äº†ä»£ç†ï¼Œå¹¶ä¸”ä¹ŸçŸ¥é“ä½ çš„çœŸå®IPã€‚
		2.ä½¿ç”¨åŒ¿åä»£ç†ï¼Œå¯¹æ–¹æœåŠ¡å™¨å¯ä»¥çŸ¥é“ä½ ä½¿ç”¨äº†ä»£ç†ï¼Œä½†ä¸çŸ¥é“ä½ çš„çœŸå®IPã€‚
		3.ä½¿ç”¨é«˜åŒ¿åä»£ç†ï¼Œå¯¹æ–¹æœåŠ¡å™¨ä¸çŸ¥é“ä½ ä½¿ç”¨äº†ä»£ç†ï¼Œæ›´ä¸çŸ¥é“ä½ çš„çœŸå®IPã€‚
3.éªŒè¯ç è®¿é—®
	æ‰“ç å¹³å°
      äº‘æ‰“ç å¹³å°
      è¶…çº§ğŸ¦…
4.åŠ¨æ€åŠ è½½ç½‘é¡µ  ç½‘ç«™è¿”å›çš„æ˜¯jsæ•°æ® å¹¶ä¸æ˜¯ç½‘é¡µçš„çœŸå®æ•°æ® 
	seleniumé©±åŠ¨çœŸå®çš„æµè§ˆå™¨å‘é€è¯·æ±‚
5.æ•°æ®åŠ å¯†  
	åˆ†æjsä»£ç 
	
çˆ¬è™«-åçˆ¬è™«-ååçˆ¬è™«
```

#### Httpåè®®

```
1.httpå’ŒhttpsåŒºåˆ«ï¼Ÿ
	http
		æ˜æ–‡ä¼ è¾“ï¼Œç«¯å£å·80
		HTTPåè®®ï¼ˆHyperText Transfer Protocolï¼Œè¶…æ–‡æœ¬ä¼ è¾“åè®®ï¼‰ï¼šæ˜¯ä¸€ç§å‘å¸ƒå’Œæ¥æ”¶ HTMLé¡µé¢çš„æ–¹æ³•ã€‚

	https
		åŠ å¯†ä¼ è¾“ï¼Œç«¯å£å·443
		HTTPSï¼ˆHypertext Transfer Protocol over Secure Socket Layerï¼‰ç®€å•è®²æ˜¯HTTPçš„å®‰å…¨ç‰ˆï¼Œåœ¨HTTPä¸‹åŠ å…¥SSLå±‚ã€‚    HTTPS = HTTP+SSL

		SSLï¼ˆSecure Sockets Layer å®‰å…¨ å¥—æ¥å±‚ï¼‰ä¸»è¦ç”¨äºWebçš„å®‰å…¨ä¼ è¾“åè®®ï¼Œåœ¨ä¼ è¾“å±‚å¯¹ç½‘ç»œè¿æ¥è¿›è¡ŒåŠ å¯†ï¼Œä¿éšœåœ¨Internetä¸Šæ•°æ®ä¼ è¾“çš„å®‰å…¨ã€‚
2.ä»€ä¹ˆæ˜¯SSLï¼Ÿ
	SSL
		ä»€ä¹ˆæ˜¯å®‰å…¨è®¤è¯
		å…³äºCA
		12306ç½‘ç«™è¯ä¹¦æ˜¯è‡ªå·±çš„
		å®‰å…¨è®¤è¯requests
		å®‰å…¨è®¤è¯urllib
	æ³¨æ„ï¼šå¦‚æœæŠ¥é”™SSL,é‚£ä¹ˆè§£å†³æ–¹æ¡ˆæ˜¯
              import urllib.request
              import ssl
              ssl._create_default_https_context = ssl._create_unverified_context
3.å¸¸è§æœåŠ¡å™¨ç«¯å£å·
		ftp      21
		ssh      22
		mysql    3306
		oracle   1521
		MongoDB  27017
		redis    6379
	httpå·¥ä½œåŸç†
		urlç»„æˆ
			åè®®	ä¸»æœº   ç«¯å£å·  è·¯å¾„   å‚æ•°  é”šç‚¹
		ä¸Šç½‘åŸç†
		httpè¯·æ±‚å’Œå“åº”
			è¯·æ±‚è¡Œ.è¯·æ±‚å¤´ã€è¯·æ±‚ä½“
			å“åº”è¡Œ.å“åº”å¤´ã€å“åº”ä½“
			è¯·æ±‚å¤´è¯¦è§£
				Accept
				Accept-Encoding
				Accept-Language
				Cache-Control
				Connection
				Cookie
				Host
				Upgrade-Insecure-Requests    httpæ˜¯å¦å‡çº§ä¸ºhttps
				User-Agent 
				X-Requested-With             æ˜¯å¦æ˜¯ajaxè¯·æ±‚             
				Referer                      ä¸Šä¸€çº§è·¯å¾„
			å“åº”å¤´è¯¦è§£
				Connection
				Content-Encoding
				Content-Type
				Date
				Expires
				Server
				Transfer-Encoding            å†…å®¹æ˜¯å¦åˆ†åŒ…ä¼ è¾“
			å¸¸è§HTTPçŠ¶æ€ç 
				200
					è¯·æ±‚æˆåŠŸ
				404
					æœªæ‰¾åˆ°èµ„æº
				500
					æœåŠ¡å™¨å†…éƒ¨é”™è¯¯
```



#### urllibåº“ä½¿ç”¨

```
	urllib.request.urlopen() æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
	response    æœåŠ¡å™¨è¿”å›çš„æ•°æ®
		responseçš„æ•°æ®ç±»å‹æ˜¯HttpResponse
		å­—èŠ‚-->å­—ç¬¦ä¸²
				è§£ç decode
		å­—ç¬¦ä¸²-->å­—èŠ‚
				ç¼–ç encode
		read()       å­—èŠ‚å½¢å¼è¯»å–äºŒè¿›åˆ¶   æ‰©å±•ï¼šrede(5)è¿”å›å‰å‡ ä¸ªå­—èŠ‚
		readline()   è¯»å–ä¸€è¡Œ
		readlines()  ä¸€è¡Œä¸€è¡Œè¯»å– ç›´è‡³ç»“æŸ
		getcode()    è·å–çŠ¶æ€ç 
		geturl()     è·å–url
		getheaders() è·å–headers
	urllib.request.urlretrieve()
		è¯·æ±‚ç½‘é¡µ
		è¯·æ±‚å›¾ç‰‡
		è¯·æ±‚è§†é¢‘
```

```
pycharmä»£ç ä¾‹å­ï¼š

import urllib.request
url='http://www.baidu.com'
# ä½¿ç”¨urllibè®¿é—®urlè·¯å¾„
response = urllib.request.urlopen(url=url)
# å¾—åˆ°responseå³ä½¿è®¿é—®è·¯å¾„çš„å“åº”,å¯ä»¥ä½¿ç”¨æ–¹æ³•è¯»å–å†…å®¹
content = response.read()  
# éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¯»å–çš„å†…å®¹æ˜¯äºŒè¿›åˆ¶ï¼Œ
# ä¸€èˆ¬çš„å†…å®¹ç¼–ç éƒ½æ˜¯utf-8 
# ä¹Ÿæœ‰è‡ªå®šä¹‰ç¼–ç çš„ç½‘é¡µï¼Œæ ¹æ®ç½‘é¡µä¸­metaçš„charsetæ¥è§£ç 
content = content.decode('utf-8')
# æ­¤æ—¶çš„contentå°±æ˜¯è®¿é—®urlå¾—åˆ°çš„å†…å®¹
# è®¸å¤šç½‘é¡µæ·»åŠ äº†åæ‰’æ‰‹æ®µï¼Œä»…ä»…ä½¿ç”¨è¿™äº›ä»£ç å¹¶ä¸èƒ½å¾—åˆ°çœŸæ­£çš„æºç 

#ä½¿ç”¨urllibä¸‹è½½å›¾ç‰‡ï¼Œä¸‹è½½è§†é¢‘ã€éŸ³é¢‘
url=`src`
# è¿™é‡Œçš„srcæ˜¯å›¾ç‰‡æˆ–è€…è§†é¢‘èµ„æºçš„ä¸‹è½½è·¯å¾„

urllib.request.urlretrieve(url=url,filename='diy.jpg')
#æ­¤æ—¶ä¼šå°†è·¯å¾„å¾—åˆ°çš„å†…å®¹ä¸‹è½½åˆ°æœ¬åœ°ï¼Œå¹¶ä¸€filenameå‘½å

```



æ‰©å±•ï¼šç¼–ç çš„ç”±æ¥

```
'''ç¼–ç é›†çš„æ¼”å˜---
ç”±äºè®¡ç®—æœºæ˜¯ç¾å›½äººå‘æ˜çš„ï¼Œå› æ­¤ï¼Œæœ€æ—©åªæœ‰127ä¸ªå­—ç¬¦è¢«ç¼–ç åˆ°è®¡ç®—æœºé‡Œï¼Œä¹Ÿå°±æ˜¯å¤§å°å†™è‹±æ–‡å­—æ¯ã€æ•°å­—å’Œä¸€äº›ç¬¦å·ï¼Œ
è¿™ä¸ªç¼–ç è¡¨è¢«ç§°ä¸ºASCIIç¼–ç ï¼Œæ¯”å¦‚å¤§å†™å­—æ¯Açš„ç¼–ç æ˜¯65ï¼Œå°å†™å­—æ¯zçš„ç¼–ç æ˜¯122ï¼Œ0çš„ç¼–ç æ˜¯48ã€‚
ä½†æ˜¯è¦å¤„ç†ä¸­æ–‡æ˜¾ç„¶ä¸€ä¸ªå­—èŠ‚æ˜¯ä¸å¤Ÿçš„ï¼Œè‡³å°‘éœ€è¦ä¸¤ä¸ªå­—èŠ‚ï¼Œè€Œä¸”è¿˜ä¸èƒ½å’ŒASCIIç¼–ç å†²çªï¼Œ
æ‰€ä»¥ï¼Œä¸­å›½åˆ¶å®šäº†GB2312ç¼–ç ï¼Œç”¨æ¥æŠŠä¸­æ–‡ç¼–è¿›å»ã€‚
ä½ å¯ä»¥æƒ³å¾—åˆ°çš„æ˜¯ï¼Œå…¨ä¸–ç•Œæœ‰ä¸Šç™¾ç§è¯­è¨€ï¼Œæ—¥æœ¬æŠŠæ—¥æ–‡ç¼–åˆ°Shift_JISé‡Œï¼ŒéŸ©å›½æŠŠéŸ©æ–‡ç¼–åˆ°Euc-kré‡Œï¼Œ
å„å›½æœ‰å„å›½çš„æ ‡å‡†ï¼Œå°±ä¼šä¸å¯é¿å…åœ°å‡ºç°å†²çªï¼Œç»“æœå°±æ˜¯ï¼Œåœ¨å¤šè¯­è¨€æ··åˆçš„æ–‡æœ¬ä¸­ï¼Œæ˜¾ç¤ºå‡ºæ¥ä¼šæœ‰ä¹±ç ã€‚
å› æ­¤ï¼ŒUnicodeåº”è¿è€Œç”Ÿã€‚UnicodeæŠŠæ‰€æœ‰è¯­è¨€éƒ½ç»Ÿä¸€åˆ°ä¸€å¥—ç¼–ç é‡Œï¼Œè¿™æ ·å°±ä¸ä¼šå†æœ‰ä¹±ç é—®é¢˜äº†ã€‚
Unicodeæ ‡å‡†ä¹Ÿåœ¨ä¸æ–­å‘å±•ï¼Œä½†æœ€å¸¸ç”¨çš„æ˜¯ç”¨ä¸¤ä¸ªå­—èŠ‚è¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ï¼ˆå¦‚æœè¦ç”¨åˆ°éå¸¸ååƒ»çš„å­—ç¬¦ï¼Œå°±éœ€è¦4ä¸ªå­—èŠ‚ï¼‰ã€‚
ç°ä»£æ“ä½œç³»ç»Ÿå’Œå¤§å¤šæ•°ç¼–ç¨‹è¯­è¨€éƒ½ç›´æ¥æ”¯æŒUnicodeã€‚'''

```

#### è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶

```
UAä»‹ç»ï¼šUser Agentä¸­æ–‡åä¸ºç”¨æˆ·ä»£ç†ï¼Œç®€ç§° UAï¼Œå®ƒæ˜¯ä¸€ä¸ªç‰¹æ®Šå­—ç¬¦ä¸²å¤´ï¼Œä½¿å¾—æœåŠ¡å™¨èƒ½å¤Ÿè¯†åˆ«å®¢æˆ·ä½¿ç”¨çš„æ“ä½œç³»ç»ŸåŠç‰ˆæœ¬ã€CPU ç±»å‹ã€æµè§ˆå™¨åŠç‰ˆæœ¬ã€‚æµè§ˆå™¨å†…æ ¸ã€æµè§ˆå™¨æ¸²æŸ“å¼•æ“ã€æµè§ˆå™¨è¯­è¨€ã€æµè§ˆå™¨æ’ä»¶ç­‰

```

```
è¯­æ³•ï¼šrequest = urllib.request.Request()

```



#### 1ç¼–è§£ç 

###### 1.getè¯·æ±‚æ–¹å¼ï¼šurllib.parse.quoteï¼ˆï¼‰

```
egï¼š
import urllib.request
import urllib.parse

url = 'https://www.baidu.com/s?wd='

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'
}

url = url + urllib.parse.quote('å°é‡')

request = urllib.request.Request(url=url,headers=headers)

response = urllib.request.urlopen(request)

print(response.read().decode('utf-8'))

```

```
ajaxçš„getè¯·æ±‚ä¾‹å­ï¼š

import urllib.request 
import urllib.parse

url = 'http://â€¦â€¦'

data={
	'kw':'data1',
	'wd':'data2',
}

headers = {
	    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36',

}

# ä½¿ç”¨urllib.parseè¿›è¡Œè½¬ç 
# quote è¿›è¡Œå•ä¸ªæ•°æ®è½¬ç 
# data = urllib.parse.quote('æœç´¢å†…å®¹') 


```













###### 2.getè¯·æ±‚æ–¹å¼ï¼šurllib.parse.urlencodeï¼ˆï¼‰

```
eg:
import urllib.request
import urllib.parse
url = 'http://www.baidu.com/s?'
data = {
    'name':'å°åˆš',
    'sex':'ç”·',
}
data = urllib.parse.urlencode(data)
url = url + data
print(url)
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'
}
request = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(request)
print(response.read().decode('utf-8'))

```

###### 3.postè¯·æ±‚æ–¹å¼

```
eg:ç™¾åº¦ç¿»è¯‘
import urllib.request
import urllib.parse
url = 'https://fanyi.baidu.com/sug'
headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'
}
keyword = input('è¯·è¾“å…¥æ‚¨è¦æŸ¥è¯¢çš„å•è¯')
data = {
    'kw':keyword
}
data = urllib.parse.urlencode(data).encode('utf-8')
request = urllib.request.Request(url=url,headers=headers,data=data)
response = urllib.request.urlopen(request)
print(response.read().decode('utf-8'))

```

æ€»ç»“ï¼špostå’ŒgetåŒºåˆ«ï¼Ÿ

```
1ï¼šgetè¯·æ±‚æ–¹å¼çš„å‚æ•°å¿…é¡»ç¼–ç ï¼Œå‚æ•°æ˜¯æ‹¼æ¥åˆ°urlåé¢ï¼Œç¼–ç ä¹‹åä¸éœ€è¦è°ƒç”¨encodeæ–¹æ³•
2ï¼špostè¯·æ±‚æ–¹å¼çš„å‚æ•°å¿…é¡»ç¼–ç ï¼Œå‚æ•°æ˜¯æ”¾åœ¨è¯·æ±‚å¯¹è±¡å®šåˆ¶çš„æ–¹æ³•ä¸­ï¼Œç¼–ç ä¹‹åéœ€è¦è°ƒç”¨encodeæ–¹æ³•

```

æ¡ˆä¾‹ç»ƒä¹ ï¼šç™¾åº¦è¯¦ç»†ç¿»è¯‘

```
import urllib.request
import urllib.parse

url = 'https://fanyi.baidu.com/v2transapi'
headers = {
    # ':authority': 'fanyi.baidu.com',
    # ':method': 'POST',
    # ':path': '/v2transapi',
    # ':scheme': 'https',
    # 'accept': '*/*',
    # 'accept-encoding': 'gzip, deflate, br',
    # 'accept-language': 'zh-CN,zh;q=0.9',
    # 'content-length': '119',
    # 'content-type': 'application/x-www-form-urlencoded; charset=UTF-8',
    'cookie': 'REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; PSTM=1537097513; BIDUPSID=D96F9A49A8630C54630DD60CE082A55C; BAIDUID=0814C35D13AE23F5EAFA8E0B24D9B436:FG=1; to_lang_often=%5B%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%2C%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%5D; from_lang_often=%5B%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%2C%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%5D; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; delPer=0; H_PS_PSSID=1424_21115_29522_29519_29099_29568_28835_29220_26350; PSINO=2; locale=zh; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1563000604,1563334706,1565592510; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1565592510; yjs_js_security_passport=2379b52646498f3b5d216e6b21c6f1c7bf00f062_1565592544_js',
    # 'origin': 'https://fanyi.baidu.com',
    # 'referer': 'https://fanyi.baidu.com/translate?aldtype=16047&query=&keyfrom=baidu&smartresult=dict&lang=auto2zh',
    # 'sec-fetch-mode': 'cors',
    # 'sec-fetch-site': 'same-origin',
    # 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36',
    # 'x-requested-with': 'XMLHttpRequest',
}
data = {
    'from': 'en',
    'to': 'zh',
    'query': 'you',
    'transtype': 'realtime',
    'simple_means_flag': '3',
    'sign': '269482.65435',
    'token': '2e0f1cb44414248f3a2b49fbad28bbd5',
}
#å‚æ•°çš„ç¼–ç 
data = urllib.parse.urlencode(data).encode('utf-8')
# è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
request = urllib.request.Request(url=url,headers=headers,data=data)
response = urllib.request.urlopen(request)
# è¯·æ±‚ä¹‹åè¿”å›çš„æ‰€æœ‰çš„æ•°æ®
content = response.read().decode('utf-8')
import json
# loadså°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºpythonå¯¹è±¡
obj = json.loads(content)
# pythonå¯¹è±¡è½¬æ¢ä¸ºjsonå­—ç¬¦ä¸²  ensure_ascii=False  å¿½ç•¥å­—ç¬¦é›†ç¼–ç 
s = json.dumps(obj,ensure_ascii=False)
print(s)

```

#### ajaxçš„getè¯·æ±‚

æ¡ˆä¾‹ï¼šè±†ç“£ç”µå½±

```
# çˆ¬å–è±†ç“£ç”µå½±å‰10é¡µæ•°æ®
# https://movie.douban.com/j/chart/top_list?type=20&interval_id=100%3A90&action=&start=0&limit=20
# https://movie.douban.com/j/chart/top_list?type=20&interval_id=100%3A90&action=&start=20&limit=20
# https://movie.douban.com/j/chart/top_list?type=20&interval_id=100%3A90&action=&start=40&limit=20

import urllib.request
import urllib.parse

# ä¸‹è½½å‰10é¡µæ•°æ®
# ä¸‹è½½çš„æ­¥éª¤ï¼š1.è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶  2.è·å–å“åº”çš„æ•°æ® 3.ä¸‹è½½

# æ¯æ‰§è¡Œä¸€æ¬¡è¿”å›ä¸€ä¸ªrequestå¯¹è±¡
def create_request(page):
    base_url = 'https://movie.douban.com/j/chart/top_list?type=20&interval_id=100%3A90&action=&'
    headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'
    }
    data={
        #  1 2  3  4
        #  0 20 40 60
        'start':(page-1)*20,
        'limit':20
    }
    # dataç¼–ç 
    data = urllib.parse.urlencode(data)
    url = base_url + data
    request = urllib.request.Request(url=url,headers=headers)
    return request
# è·å–ç½‘é¡µæºç 
def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    return content

def down_load(page,content):
#    with openï¼ˆæ–‡ä»¶çš„åå­—ï¼Œæ¨¡å¼ï¼Œç¼–ç ï¼‰as fp:
#        fp.write(å†…å®¹)
    with open('douban_'+str(page)+'.json','w',encoding='utf-8')as fp:
        fp.write(content)

if __name__ == '__main__':
    start_page = int(input('è¯·è¾“å…¥èµ·å§‹é¡µç '))
    end_page = int(input('è¯·è¾“å…¥ç»“æŸé¡µç '))
    for page in range(start_page,end_page+1):
        request = create_request(page)
        content = get_content(request)
        down_load(page,content)

```

#### ajaxçš„postè¯·æ±‚

æ¡ˆä¾‹ï¼šKFCå®˜ç½‘

#### å¤æ‚get

æ¡ˆä¾‹ï¼šç™¾åº¦è´´å§



æ‰©å±•ï¼šCURLï¼ˆè®¿é—®æœåŠ¡å™¨è·å–é¡µé¢æºç ï¼‰

```
1.æŸ¥çœ‹curlç‰ˆæœ¬
2.curlåŸºæœ¬ä½¿ç”¨
		ï¼ˆ1ï¼‰è®¿é—®é¡µé¢ egï¼šcurl http://www.baidu.com
		 (2)æºå¸¦UA   eg: curl -A 'Chrome' http://www.baidu.com
		 (3)postè¯·æ±‚ egï¼šcurl -X POST http://httpbin.org/post

```

### URLError\HTTPError

```
ç®€ä»‹:1.HTTPErrorç±»æ˜¯URLErrorç±»çš„å­ç±»
     2.å¯¼å…¥çš„åŒ…urllib.error.HTTPError    urllib.error.URLError
     3.httpé”™è¯¯ï¼šhttpé”™è¯¯æ˜¯é’ˆå¯¹æµè§ˆå™¨æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨è€Œå¢åŠ å‡ºæ¥çš„é”™è¯¯æç¤ºã€‚å¼•å¯¼å¹¶å‘Šè¯‰æµè§ˆè€…è¯¥é¡µæ˜¯å“ªé‡Œå‡ºäº†é—®é¢˜ã€‚
     4.é€šè¿‡urllibå‘é€è¯·æ±‚çš„æ—¶å€™ï¼Œæœ‰å¯èƒ½ä¼šå‘é€å¤±è´¥ï¼Œè¿™ä¸ªæ—¶å€™å¦‚æœæƒ³è®©ä½ çš„ä»£ç æ›´åŠ çš„å¥å£®ï¼Œå¯ä»¥é€šè¿‡try-exceptè¿›è¡Œæ•è·å¼‚å¸¸ï¼Œå¼‚å¸¸æœ‰ä¸¤ç±»ï¼ŒURLError\HTTPError
```

```
eg:

import urllib.request
import urllib.error

url = 'https://blog.csdn.net/ityard/article/details/102646738'

# url = 'http://www.goudan11111.com'

headers = {
        # 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',
        # 'Accept-Encoding': 'gzip, deflate, br',
        # 'Accept-Language': 'zh-CN,zh;q=0.9',
        # 'Cache-Control': 'max-age=0',
        # 'Connection': 'keep-alive',
        'Cookie': 'uuid_tt_dd=10_19284691370-',
        # 'Host': 'blog.csdn.net',
        # 'Referer': 'https://passport.csdn.net/login?code=public',
        # 'Sec-Fetch-Mode': 'navigate',
        # 'Sec-Fetch-Site': 'same-site',
        # 'Sec-Fetch-User': '?1',
        # 'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36',
    }
try:
    request = urllib.request.Request(url=url,headers=headers)

    response = urllib.request.urlopen(request)

    content = response.read().decode('utf-8')
    print(content)
except urllib.error.HTTPError:
    print(1111)

except urllib.error.URLError:
    print(2222)

```

### cookieç™»å½•

```
ä½¿ç”¨æ¡ˆä¾‹ï¼š
        1.äººäººç½‘ç™»é™†
        2.weiboç™»é™†
        ä½œä¸šï¼šqqç©ºé—´çš„çˆ¬å–
åœ¨å¤´ä¿¡æ¯ä¸­æ·»åŠ cookieï¼Œæºå¸¦cookieä¿¡æ¯
```

### Handlerå¤„ç†å™¨

```
ä¸ºä»€ä¹ˆè¦å­¦ä¹ handlerï¼Ÿ
      urllib.request.urlopen(url)
          ä¸èƒ½å®šåˆ¶è¯·æ±‚å¤´
      urllib.request.Request(url,headers,data)
          å¯ä»¥å®šåˆ¶è¯·æ±‚å¤´
      Handler
          å®šåˆ¶æ›´é«˜çº§çš„è¯·æ±‚å¤´ï¼ˆéšç€ä¸šåŠ¡é€»è¾‘çš„å¤æ‚ è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶å·²ç»æ»¡è¶³ä¸äº†æˆ‘ä»¬çš„éœ€æ±‚ï¼ˆåŠ¨æ€cookieå’Œä»£ç†ä¸èƒ½ä½¿ç”¨è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶ï¼‰
```

```
eg:
import urllib.request
url = 'http://www.baidu.com'
headers = {
        'User - Agent': 'Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 74.0.3729.169Safari / 537.36'
    }
request = urllib.request.Request(url=url,headers=headers)

handler = urllib.request.HTTPHandler()

opener = urllib.request.build_opener(handler)

response = opener.open(request)

print(response.read().decode('utf-8'))

```

### ä»£ç†æœåŠ¡å™¨

```
1.ä»€ä¹ˆæ˜¯ä»£ç†æœåŠ¡å™¨?
	æ˜¯ä¸€ç§é‡è¦çš„æœåŠ¡å™¨å®‰å…¨åŠŸèƒ½ï¼Œå®ƒçš„å·¥ä½œä¸»è¦åœ¨å¼€æ”¾ç³»ç»Ÿäº’è”(OSI)æ¨¡å‹çš„ä¼šè¯å±‚ï¼Œä»è€Œèµ·åˆ°é˜²ç«å¢™çš„ä½œç”¨
ç¿»å¢™ï¼Œæ˜¯æŒ‡ç»•è¿‡ç›¸åº”çš„IPå°é”ã€å†…å®¹è¿‡æ»¤ã€åŸŸååŠ«æŒã€æµé‡é™åˆ¶ç­‰
2.ä»£ç†çš„å¸¸ç”¨åŠŸèƒ½?
	1.çªç ´è‡ªèº«IPè®¿é—®é™åˆ¶ï¼Œè®¿é—®å›½å¤–ç«™ç‚¹ã€‚
	2.è®¿é—®ä¸€äº›å•ä½æˆ–å›¢ä½“å†…éƒ¨èµ„æº
			æ‰©å±•ï¼šæŸå¤§å­¦FTP(å‰ææ˜¯è¯¥ä»£ç†åœ°å€åœ¨è¯¥èµ„æºçš„å…è®¸è®¿é—®èŒƒå›´ä¹‹å†…)ï¼Œä½¿ç”¨æ•™è‚²ç½‘å†…åœ°å€æ®µå…è´¹ä»£ç†æœåŠ¡å™¨ï¼Œå°±å¯ä»¥ç”¨äºå¯¹æ•™è‚²ç½‘å¼€æ”¾çš„å„ç±»FTPä¸‹è½½ä¸Šä¼ ï¼Œä»¥åŠå„ç±»èµ„æ–™æŸ¥è¯¢å…±äº«ç­‰æœåŠ¡ã€‚
	3.æé«˜è®¿é—®é€Ÿåº¦
			æ‰©å±•ï¼šé€šå¸¸ä»£ç†æœåŠ¡å™¨éƒ½è®¾ç½®ä¸€ä¸ªè¾ƒå¤§çš„ç¡¬ç›˜ç¼“å†²åŒºï¼Œå½“æœ‰å¤–ç•Œçš„ä¿¡æ¯é€šè¿‡æ—¶ï¼ŒåŒæ—¶ä¹Ÿå°†å…¶ä¿å­˜åˆ°ç¼“å†²åŒºä¸­ï¼Œå½“å…¶ä»–ç”¨æˆ·å†è®¿é—®ç›¸åŒçš„ä¿¡æ¯æ—¶ï¼Œ åˆ™ç›´æ¥ç”±ç¼“å†²åŒºä¸­å–å‡ºä¿¡æ¯ï¼Œä¼ ç»™ç”¨æˆ·ï¼Œä»¥æé«˜è®¿é—®é€Ÿåº¦ã€‚
	4.éšè—çœŸå®IP
			æ‰©å±•ï¼šä¸Šç½‘è€…ä¹Ÿå¯ä»¥é€šè¿‡è¿™ç§æ–¹æ³•éšè—è‡ªå·±çš„IPï¼Œå…å—æ”»å‡»ã€‚
3.ä»£ç é…ç½®ä»£ç†
	åˆ›å»ºReuqestå¯¹è±¡
	åˆ›å»ºProxyHandlerå¯¹è±¡
	ç”¨handlerå¯¹è±¡åˆ›å»ºopenerå¯¹è±¡
	ä½¿ç”¨opener.openå‡½æ•°å‘é€è¯·æ±‚
```

```python
eg:
import urllib.request
url = 'http://www.baidu.com/s?wd=ip'
headers = {
	'User - Agent': 'Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 74.0.3729.169Safari / 537.36',
    'cookie':'dfajhgla',
    }
request = urllib.request.Request(url=url,headers=headers)
proxies = {'http':'117.141.155.244:53281'}
handler = urllib.request.ProxyHandler(proxies=proxies)
opener = urllib.request.build_opener(handler)
response = opener.open(request)
content = response.read().decode('utf-8')
with open('daili.html','w',encoding='utf-8')as fp:
    fp.write(content)
```

æ‰©å±•ï¼š1.ä»£ç†æ± ï¼Œåœ¨ä¸€ä¸ªå­—å…¸ä¸­éšæœºé€‰ä¸€ä¸ª

â€‹			2.å¿«ä»£ç†ï¼ˆç½‘ç«™ï¼‰



### cookieåº“

	1.cookieåº“èƒ½å¹²å•¥:é€šè¿‡handlerç™»é™†ä¼šè‡ªåŠ¨çš„ä¿å­˜ç™»å½•ä¹‹åçš„cookie
	2.cookieåº“é…ç½®
		åˆ›å»ºä¸€ä¸ªCookieJarå¯¹è±¡
		ä½¿ç”¨cookiejarå¯¹è±¡ï¼Œåˆ›å»ºä¸€ä¸ªhandlerå¯¹è±¡
		ä½¿ç”¨handleråˆ›å»ºä¸€ä¸ªopener
		é€šè¿‡openerç™»å½•
		handlerä¼šè‡ªåŠ¨çš„ä¿å­˜ç™»å½•ä¹‹åçš„cookie

```python
æ¡ˆä¾‹ï¼šå…¨ä¹¦ç½‘
# ç™»é™†ä¹‹åè¿›å…¥åˆ°è—ä¹¦æ¶
import urllib.request
import urllib.parse
import http.cookiejar

url_login = 'http://www.quanshuwang.com/login.php?do=submit'
headers = {
        'User - Agent': 'Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 74.0.3729.169Safari / 537.36'
    }
data = {
    'username': 'action',
    'password': 'action',
    'action': 'login',
}
# å¯¹å‚æ•°è¿›è¡Œç¼–ç 
data = urllib.parse.urlencode(data).encode('utf-8')
# è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
request = urllib.request.Request(url=url_login,headers=headers,data=data)
# è·å–ä¸€ä¸ªcookiejarå¯¹è±¡
cookiejar = http.cookiejar.CookieJar()
# è·å–ä¸€ä¸ªhandlerå¯¹è±¡
handler = urllib.request.HTTPCookieProcessor(cookiejar=cookiejar)
# è·å–openerå¯¹è±¡
opener = urllib.request.build_opener(handler)
# è·å–responseå¯¹è±¡
response = opener.open(request)
url_bookcase = 'http://www.quanshuwang.com/modules/article/bookcase.php'
request_bookcase = urllib.request.Request(url=url_bookcase,headers=headers)
# å› ä¸ºä¹‹å‰çš„openerä¸­ å·²ç»åŒ…å«äº†ç™»é™†çš„cookie  é‚£ä¹ˆå†æ¬¡ä½¿ç”¨opener å°±ä¼šæºå¸¦å·²ç»å­˜åœ¨cookie
# å»è®¿é—®äº†
response_bookcase = opener.open(request_bookcase)
content = response_bookcase.read().decode('gbk')
with open('bookcase.html','w',encoding='gbk')as fp:
    fp.write(content)
```



### xpath

```xpath
xpathåŸºæœ¬è¯­æ³•ï¼š
	1.è·¯å¾„æŸ¥è¯¢
		//ï¼šæŸ¥æ‰¾æ‰€æœ‰å­å­™èŠ‚ç‚¹ï¼Œä¸è€ƒè™‘å±‚çº§å…³ç³»
		/ï¼šæ‰¾ç›´æ¥å­èŠ‚ç‚¹
	2.è°“è¯æŸ¥è¯¢
		//div[@id]  
		//div[@id="maincontent"]    
	3.å±æ€§æŸ¥è¯¢
		//@class         
	4.æ¨¡ç³ŠæŸ¥è¯¢
		//div[contains(@id, "he")]   
		//div[starts-with(@id, "he")] 
	5.å†…å®¹æŸ¥è¯¢
		//div/h1/text()
	6.é€»è¾‘è¿ç®—
		//div[@id="head" and @class="s_down"]
		//title | //price
```

```python
xpathä½¿ç”¨ï¼š
	æ³¨æ„ï¼šæå‰å®‰è£…xpathæ’ä»¶
	1.å®‰è£…lxmlåº“      
			pip install lxml -i https://pypi.douban.com/simple
	2.å¯¼å…¥lxml.etree  
			from lxml import etree
	3.etree.parse()   è§£ææœ¬åœ°æ–‡ä»¶
		    html_tree = etree.parse('XX.html')	
	4.etree.HTML()    æœåŠ¡å™¨å“åº”æ–‡ä»¶
		    html_tree = etree.HTML(response.read().decode('utf-8')	
	4.html_tree.xpath(xpathè·¯å¾„)
```



### JsonPath

```
jsonpathçš„å®‰è£…åŠä½¿ç”¨æ–¹å¼ï¼š
			pipå®‰è£…ï¼š 
				   pip install jsonpath
			jsonpathçš„ä½¿ç”¨ï¼š
                    obj = json.load(open('jsonæ–‡ä»¶', 'r', encoding='utf-8'))
                    ret = jsonpath.jsonpath(obj, 'jsonpathè¯­æ³•')
```

```
jsonå¯¹è±¡çš„è½¬æ¢
	json.loads()
		æ˜¯å°†å­—ç¬¦ä¸²è½¬åŒ–ä¸ºpythonå¯¹è±¡
	json.dumps()
		å°†pythonå¯¹è±¡è½¬åŒ–ä¸ºjsonæ ¼å¼çš„å­—ç¬¦ä¸²
	json.load()
		è¯»å–jsonæ ¼å¼çš„æ–‡æœ¬ï¼Œè½¬åŒ–ä¸ºpythonå¯¹è±¡
		json.load(open(a.json))
	json.dump()
		å°†pythonå¯¹è±¡å†™å…¥åˆ°æ–‡æœ¬ä¸­
```

æ•™ç¨‹è¿æ¥ï¼ˆhttp://blog.csdn.net/luxideyao/article/details/77802389ï¼‰

åº”ç”¨æ¡ˆä¾‹ï¼šæ™ºè”æ‹›è˜ï¼ˆè–ªæ°´ï¼Œå…¬å¸åç§°ï¼ŒèŒä½éœ€æ±‚ï¼‰

ä½œä¸šï¼š 1.è‚¡ç¥¨ä¿¡æ¯æå–ï¼ˆhttp://quote.stockstar.com/ï¼‰



### BeautifulSoup

1.åŸºæœ¬ç®€ä»‹

```
ç®€ä»‹ï¼š
1.BeautifulSoupç®€ç§°ï¼š
     bs4
2.ä»€ä¹ˆæ˜¯BeatifulSoupï¼Ÿ
	BeautifulSoupï¼Œå’Œlxmlä¸€æ ·ï¼Œæ˜¯ä¸€ä¸ªhtmlçš„è§£æå™¨ï¼Œä¸»è¦åŠŸèƒ½ä¹Ÿæ˜¯è§£æå’Œæå–æ•°æ®
3.ä¼˜ç¼ºç‚¹ï¼Ÿ
	ç¼ºç‚¹ï¼šæ•ˆç‡æ²¡æœ‰lxmlçš„æ•ˆç‡é«˜	
	ä¼˜ç‚¹ï¼šæ¥å£è®¾è®¡äººæ€§åŒ–ï¼Œä½¿ç”¨æ–¹ä¾¿
```

2.å®‰è£…ä»¥åŠåˆ›å»º

```
1.å®‰è£…
	pip install bs4
2.å¯¼å…¥
	from bs4 import BeautifulSoup
3.åˆ›å»ºå¯¹è±¡
	æœåŠ¡å™¨å“åº”çš„æ–‡ä»¶ç”Ÿæˆå¯¹è±¡
		soup = BeautifulSoup(response.read().decode(), 'lxml')
	æœ¬åœ°æ–‡ä»¶ç”Ÿæˆå¯¹è±¡
		soup = BeautifulSoup(open('1.html'), 'lxml')
		æ³¨æ„ï¼šé»˜è®¤æ‰“å¼€æ–‡ä»¶çš„ç¼–ç æ ¼å¼gbkæ‰€ä»¥éœ€è¦æŒ‡å®šæ‰“å¼€ç¼–ç æ ¼å¼
```

3.èŠ‚ç‚¹å®šä½

```
1.æ ¹æ®æ ‡ç­¾åæŸ¥æ‰¾èŠ‚ç‚¹
		soup.a ã€æ³¨ã€‘åªèƒ½æ‰¾åˆ°ç¬¬ä¸€ä¸ªa
			soup.a.name
			soup.a.attrs
2.å‡½æ•°
		(1).find(è¿”å›ä¸€ä¸ªå¯¹è±¡)
                  find('a')ï¼šåªæ‰¾åˆ°ç¬¬ä¸€ä¸ªaæ ‡ç­¾
                  find('a', title='åå­—')
                  find('a', class_='åå­—')
		(2).find_all(è¿”å›ä¸€ä¸ªåˆ—è¡¨)
                  find_all('a')  æŸ¥æ‰¾åˆ°æ‰€æœ‰çš„a
                  find_all(['a', 'span'])  è¿”å›æ‰€æœ‰çš„aå’Œspan
                  find_all('a', limit=2)  åªæ‰¾å‰ä¸¤ä¸ªa
		(3).select(æ ¹æ®é€‰æ‹©å™¨å¾—åˆ°èŠ‚ç‚¹å¯¹è±¡)ã€æ¨èã€‘
                  1.element
                      eg:p
                  2..class
                      eg:.firstname
                  3.#id
                      eg:#firstname
                  4.å±æ€§é€‰æ‹©å™¨
                      [attribute]
                          eg:li = soup.select('li[class]')
                      [attribute=value]
                          eg:li = soup.select('li[class="hengheng1"]')
                  5.å±‚çº§é€‰æ‹©å™¨
                      element element
                          div p
                      element>element
                          div>p
                      element,element
                          div,p 
                          		eg:soup = soup.select('a,span')
3.è·å–å­å­™èŠ‚ç‚¹
		contentsï¼šè¿”å›çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨
			eg:print(soup.body.contents)
		descendantsï¼šè¿”å›çš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨
			eg:for a in soup.body.descendants:
					print(a)
```



4.èŠ‚ç‚¹ä¿¡æ¯

```
	(1).è·å–èŠ‚ç‚¹å†…å®¹ï¼šé€‚ç”¨äºæ ‡ç­¾ä¸­åµŒå¥—æ ‡ç­¾çš„ç»“æ„
            obj.string
            obj.get_text()ã€æ¨èã€‘
	(2).èŠ‚ç‚¹çš„å±æ€§
            tag.name è·å–æ ‡ç­¾å
                eg:tag = find('li)
                   print(tag.name)
		   tag.attrså°†å±æ€§å€¼ä½œä¸ºä¸€ä¸ªå­—å…¸è¿”å›
	(3).è·å–èŠ‚ç‚¹å±æ€§
            obj.attrs.get('title')ã€å¸¸ç”¨ã€‘
            obj.get('title')
            obj['title']
```

5.èŠ‚ç‚¹ç±»å‹

```
èŠ‚ç‚¹ç±»å‹
	bs4.BeautifulSoup æ ¹èŠ‚ç‚¹ç±»å‹
	bs4.element.NavigableString è¿æ¥ç±»å‹  å¯æ‰§è¡Œçš„å­—ç¬¦ä¸²
	bs4.element.Tag èŠ‚ç‚¹ç±»å‹  
	bs4.element.Comment æ³¨é‡Šç±»å‹
		eg:
			if type(aobj.string) == bs4.element.Comment:
                print('è¿™ä¸ªæ˜¯æ³¨é‡Šå†…å®¹')
             else:
                print('è¿™ä¸æ˜¯æ³¨é‡Š')
```

åº”ç”¨å®ä¾‹ï¼š 1.è‚¡ç¥¨ä¿¡æ¯æå–ï¼ˆhttp://quote.stockstar.com/ï¼‰

â€‹		    2.ä¸­åè‹±æ‰ç½‘-æ—§ç‰ˆ

â€‹                    3 .è…¾è®¯å…¬å¸æ‹›è˜éœ€æ±‚æŠ“å–ï¼ˆhttps://hr.tencent.com/index.phpï¼‰â€‹



### selenium

```
1.ä»€ä¹ˆæ˜¯seleniumï¼Ÿ
	ï¼ˆ1ï¼‰Seleniumæ˜¯ä¸€ä¸ªç”¨äºWebåº”ç”¨ç¨‹åºæµ‹è¯•çš„å·¥å…·ã€‚
	ï¼ˆ2ï¼‰Selenium æµ‹è¯•ç›´æ¥è¿è¡Œåœ¨æµè§ˆå™¨ä¸­ï¼Œå°±åƒçœŸæ­£çš„ç”¨æˆ·åœ¨æ“ä½œä¸€æ ·ã€‚
	ï¼ˆ3ï¼‰æ”¯æŒé€šè¿‡å„ç§driverï¼ˆFirfoxDriverï¼ŒIternetExplorerDriverï¼ŒOperaDriverï¼ŒChromeDriverï¼‰é©±åŠ¨çœŸå®æµè§ˆå™¨å®Œæˆæµ‹è¯•ã€‚
	ï¼ˆ4ï¼‰seleniumä¹Ÿæ˜¯æ”¯æŒæ— ç•Œé¢æµè§ˆå™¨æ“ä½œçš„ã€‚
```

```
2.ä¸ºä»€ä¹ˆä½¿ç”¨seleniumï¼Ÿ
	æ¨¡æ‹Ÿæµè§ˆå™¨åŠŸèƒ½ï¼Œè‡ªåŠ¨æ‰§è¡Œç½‘é¡µä¸­çš„jsä»£ç ï¼Œå®ç°åŠ¨æ€åŠ è½½
```

```
3.å¦‚ä½•å®‰è£…seleniumï¼Ÿ
	ï¼ˆ1ï¼‰æ“ä½œè°·æ­Œæµè§ˆå™¨é©±åŠ¨ä¸‹è½½åœ°å€
			http://chromedriver.storage.googleapis.com/index.html 
	ï¼ˆ2ï¼‰è°·æ­Œé©±åŠ¨å’Œè°·æ­Œæµè§ˆå™¨ç‰ˆæœ¬ä¹‹é—´çš„æ˜ å°„è¡¨
			http://blog.csdn.net/huilan_same/article/details/51896672
	ï¼ˆ3ï¼‰æŸ¥çœ‹è°·æ­Œæµè§ˆå™¨ç‰ˆæœ¬
			è°·æ­Œæµè§ˆå™¨å³ä¸Šè§’-->å¸®åŠ©-->å…³äº
	ï¼ˆ4ï¼‰pip install selenium
```

```
4.seleniumçš„ä½¿ç”¨æ­¥éª¤ï¼Ÿ
	ï¼ˆ1ï¼‰å¯¼å…¥ï¼šfrom selenium import webdriver
	ï¼ˆ2ï¼‰åˆ›å»ºè°·æ­Œæµè§ˆå™¨æ“ä½œå¯¹è±¡ï¼š
				path = è°·æ­Œæµè§ˆå™¨é©±åŠ¨æ–‡ä»¶è·¯å¾„
				browser = webdriver.Chrome(path)
	ï¼ˆ3ï¼‰è®¿é—®ç½‘å€
				url = è¦è®¿é—®çš„ç½‘å€
				browser.get(url)
```

```
4-1ï¼šseleniumçš„å…ƒç´ å®šä½ï¼Ÿ
		å…ƒç´ å®šä½ï¼šè‡ªåŠ¨åŒ–è¦åšçš„å°±æ˜¯æ¨¡æ‹Ÿé¼ æ ‡å’Œé”®ç›˜æ¥æ“ä½œæ¥æ“ä½œè¿™äº›å…ƒç´ ï¼Œç‚¹å‡»ã€è¾“å…¥ç­‰ç­‰ã€‚æ“ä½œè¿™äº›å…ƒç´ å‰é¦–å…ˆè¦æ‰¾åˆ°å®ƒä»¬ï¼ŒWebDriveræä¾›å¾ˆå¤šå®šä½å…ƒç´ çš„æ–¹æ³•
		æ–¹æ³•ï¼š
              1.find_element_by_id
              			eg:button = browser.find_element_by_id('su')
              2.find_elements_by_name
              			eg:name = browser.find_element_by_name('wd')
              3.find_elements_by_xpath
              			eg:xpath1 = browser.find_elements_by_xpath('//input[@id="su"]')
              4.find_elements_by_tag_name
              			eg:names = browser.find_elements_by_tag_name('input')
              5.find_elements_by_css_selector
              			eg:my_input = browser.find_elements_by_css_selector('#kw')[0]
              6.find_elements_by_link_text
              			eg:browser.find_element_by_link_text("æ–°é—»")
```

```
4-2:è®¿é—®å…ƒç´ ä¿¡æ¯
	è·å–å…ƒç´ å±æ€§
		.get_attribute('class')
	è·å–å…ƒç´ æ–‡æœ¬
		.text
	è·å–id
		.id
	è·å–æ ‡ç­¾å
		.tag_name
```

```
4-3:äº¤äº’
	ç‚¹å‡»:click()
	è¾“å…¥:send_keys()
	åé€€æ“ä½œ:browser.back()
	å‰è¿›æ“ä½œ:browser.forword()
	æ¨¡æ‹ŸJSæ»šåŠ¨:
		js = 'document.body.scrollTop=100000'
		js='document.documentElement.scrollTop=100000'
		browser.execute_script(js) æ‰§è¡Œjsä»£ç 
	è·å–ç½‘é¡µä»£ç ï¼špage_source 
	é€€å‡ºï¼šbrowser.quit()
	æ¡ˆä¾‹ç»ƒä¹ ï¼šç³—äº‹ç™¾ç§‘
```

### Phantomjs

```
1.ä»€ä¹ˆæ˜¯Phantomjsï¼Ÿ
	ï¼ˆ1ï¼‰æ˜¯ä¸€ä¸ªæ— ç•Œé¢çš„æµè§ˆå™¨
	ï¼ˆ2ï¼‰æ”¯æŒé¡µé¢å…ƒç´ æŸ¥æ‰¾ï¼Œjsçš„æ‰§è¡Œç­‰
	ï¼ˆ3ï¼‰ç”±äºä¸è¿›è¡Œcsså’Œguiæ¸²æŸ“ï¼Œè¿è¡Œæ•ˆç‡è¦æ¯”çœŸå®çš„æµè§ˆå™¨è¦å¿«å¾ˆå¤š
```

```
2.å¦‚ä½•ä½¿ç”¨Phantomjsï¼Ÿ
	ï¼ˆ1ï¼‰è·å–PhantomJS.exeæ–‡ä»¶è·¯å¾„path
	ï¼ˆ2ï¼‰browser = webdriver.PhantomJS(path)
	ï¼ˆ3ï¼‰browser.get(url)
	 æ‰©å±•ï¼šä¿å­˜å±å¹•å¿«ç…§:browser.save_screenshot('baidu.png')
```

### Chrome handless

```
1.ç³»ç»Ÿè¦æ±‚ï¼š
          Chrome 
                Unix\Linux ç³»ç»Ÿéœ€è¦ chrome >= 59 
                Windows ç³»ç»Ÿéœ€è¦ chrome >= 60
          Python3.6
          Selenium==3.4.*
          ChromeDriver==2.31
```

```
2.é…ç½®ï¼š
	from selenium import webdriver
	from selenium.webdriver.chrome.options import Options

    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--disable-gpu')

    path = r'C:\Program Files (x86)\Google\Chrome\Application\chrome.exe'
    chrome_options.binary_location = path

	browser = webdriver.Chrome(chrome_options=chrome_options)

	browser.get('http://www.baidu.com/')
```

```
3.é…ç½®å°è£…ï¼š
          from selenium import webdriver
          #è¿™ä¸ªæ˜¯æµè§ˆå™¨è‡ªå¸¦çš„  ä¸éœ€è¦æˆ‘ä»¬å†åšé¢å¤–çš„æ“ä½œ
          from selenium.webdriver.chrome.options import Options

          def share_browser():
              #åˆå§‹åŒ–
              chrome_options = Options()
              chrome_options.add_argument('--headless')
              chrome_options.add_argument('--disable-gpu')
              #æµè§ˆå™¨çš„å®‰è£…è·¯å¾„    æ‰“å¼€æ–‡ä»¶ä½ç½®
              #è¿™ä¸ªè·¯å¾„æ˜¯ä½ è°·æ­Œæµè§ˆå™¨çš„è·¯å¾„
              path = r'C:\Program Files (x86)\Google\Chrome\Application\chrome.exe'
              chrome_options.binary_location = path

              browser = webdriver.Chrome(chrome_options=chrome_options)

              return  browser
  å°è£…è°ƒç”¨ï¼š
          from handless import share_browser

          browser = share_browser()

          browser.get('http://www.baidu.com/')

          browser.save_screenshot('handless1.png')
```

### requests

```
1.æ–‡æ¡£ï¼š
	å®˜æ–¹æ–‡æ¡£
		http://cn.python-requests.org/zh_CN/latest/
	å¿«é€Ÿä¸Šæ‰‹
		http://cn.python-requests.org/zh_CN/latest/user/quickstart.html
```

```
2.å®‰è£…
	pip install requests
```

```
3.getè¯·æ±‚
	requests.get()
		eg:
			 import requests
              url = 'http://www.baidu.com/s?'
              headers = {
                  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, 				   like Gecko) Chrome/65.0.3325.181 Safari/537.36'
              }
              data = {
                  'wd':'åŒ—äº¬'
              }
              response = requests.get(url,params=data,headers=headers)
	å®šåˆ¶å‚æ•°
		å‚æ•°ä½¿ç”¨paramsä¼ é€’
		å‚æ•°æ— éœ€urlencode
	r.text : è·å–ç½‘ç«™æºç 
	r.encoding è®¿é—®æˆ–å®šåˆ¶ç¼–ç æ–¹å¼
	r.url è·å–è¯·æ±‚çš„url
	r.content å“åº”çš„å­—èŠ‚ç±»å‹
	r.status_code å“åº”çš„çŠ¶æ€ç 
	r.headers å“åº”çš„å¤´ä¿¡æ¯
```

```
4:postè¯·æ±‚
	requests.post()
	ç™¾åº¦ç¿»è¯‘:
		eg:
			import requests
			post_url = 'http://fanyi.baidu.com/sug'
             headers={
                  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 					(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'
              }
            data = {
                'kw': 'eye'
            }
			r = requests.post(url = post_url,headers=headers,data=data
```

```
5ï¼šgetå’ŒpoståŒºåˆ«ï¼Ÿ
	1ï¼šgetè¯·æ±‚çš„å‚æ•°åå­—æ˜¯params  postè¯·æ±‚çš„å‚æ•°çš„åå­—æ˜¯data  
	2 è¯·æ±‚èµ„æºè·¯å¾„åé¢å¯ä»¥ä¸åŠ ?  
	3 ä¸éœ€è¦æ‰‹åŠ¨ç¼–è§£ç   4 ä¸éœ€è¦åšè¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
```

```
6ï¼šproxyå®šåˆ¶
	åœ¨è¯·æ±‚ä¸­è®¾ç½®proxieså‚æ•°
	å‚æ•°ç±»å‹æ˜¯ä¸€ä¸ªå­—å…¸ç±»å‹
	eg:
		import requests
		url = 'http://www.baidu.com/s?'
         headers = {
              'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, 			  like Gecko) Chrome/65.0.3325.181 Safari/537.36'
          }
        data = {
            'wd':'ip'
        }
        proxy = {
              'http':'219.149.59.250:9797'
          }
	   r = requests.get(url=url,params=data,headers=headers,proxies=proxy)
        with open('proxy.html','w',encoding='utf-8') as fp:
            fp.write(r.text)
```

```
7:cookieå®šåˆ¶
	åº”ç”¨æ¡ˆä¾‹ï¼š
            ï¼ˆ1ï¼‰ç¬‘è¯é›†
            		http://www.jokeji.cn/
            		è´¦å·å¯†ç 	action   action123
            ï¼ˆ2ï¼‰å…¨ä¹¦ç½‘ç™»é™†
            		è´¦å·å¯†ç     action    action
            ï¼ˆ3ï¼‰chinaunix
            ï¼ˆ4ï¼‰å¤è¯—æ–‡ç½‘ï¼ˆéœ€è¦éªŒè¯ï¼‰
            ï¼ˆ5ï¼‰äº‘æ‰“ç å¹³å°
            	ç”¨æˆ·ç™»é™†   actionuser  action
			   å¼€å‘è€…ç™»é™†  actioncode  action
```

```
8:cookieå®šåˆ¶
headeré™„å¸¦cookie
	åº”ç”¨æ¡ˆä¾‹ï¼š
            ï¼ˆ1ï¼‰ç¬‘è¯é›†
            		http://www.jokeji.cn/
            		è´¦å·å¯†ç 	action   action123
            ï¼ˆ2ï¼‰å…¨ä¹¦ç½‘ç™»é™†
            		è´¦å·å¯†ç     action    action
            ï¼ˆ3ï¼‰å¤è¯—æ–‡ç½‘ï¼ˆéœ€è¦éªŒè¯ï¼‰
            ï¼ˆ4ï¼‰äº‘æ‰“ç å¹³å°
            	ç”¨æˆ·ç™»é™†   actionuser  action
			   å¼€å‘è€…ç™»é™†  actioncode  action
```



## scrapyæ¡†æ¶

```
1.scrapyæ˜¯ä»€ä¹ˆï¼Ÿ
		Scrapyæ˜¯ä¸€ä¸ªä¸ºäº†çˆ¬å–ç½‘ç«™æ•°æ®ï¼Œæå–ç»“æ„æ€§æ•°æ®è€Œç¼–å†™çš„åº”ç”¨æ¡†æ¶ã€‚ å¯ä»¥åº”ç”¨åœ¨åŒ…æ‹¬æ•°æ®æŒ–æ˜ï¼Œä¿¡æ¯å¤„ç†æˆ–å­˜å‚¨å†å²æ•°æ®ç­‰ä¸€ç³»åˆ—çš„ç¨‹åºä¸­ã€‚
```

```
2.å®‰è£…scrapyï¼š
			pip install scrapy
  å®‰è£…è¿‡ç¨‹ä¸­å‡ºé”™ï¼š
  			å¦‚æœå®‰è£…æœ‰é”™è¯¯ï¼ï¼ï¼ï¼
             pip install Scrapy
             building 'twisted.test.raiser' extension
             error: Microsoft Visual C++ 14.0 is required. Get it with "Microsoft Visual C++ 			 Build Tools": http://landinghub.visualstudio.com/visual-cpp-build-tools
  è§£å†³æ–¹æ¡ˆï¼š
		http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted 
		ä¸‹è½½twistedå¯¹åº”ç‰ˆæœ¬çš„whlæ–‡ä»¶ï¼ˆå¦‚æˆ‘çš„Twisted-17.5.0-cp36-cp36m-win_amd64.whlï¼‰ï¼Œcpåé¢æ˜¯            pythonç‰ˆæœ¬ï¼Œamd64ä»£è¡¨64ä½ï¼Œè¿è¡Œå‘½ä»¤ï¼š
		pip install C:\Users\...\Twisted-17.5.0-cp36-cp36m-win_amd64.whl
		pip install Scrapy
  å¦‚æœå†æŠ¥é”™   win32
  è§£å†³æ–¹æ³•ï¼š
  		pip install pypiwin32
  å†æŠ¥é”™ï¼šä½¿ç”¨anaconda
```

### scrapyé¡¹ç›®çš„åˆ›å»ºä»¥åŠè¿è¡Œ

```
1.åˆ›å»ºscrapyé¡¹ç›®ï¼š
			  ç»ˆç«¯è¾“å…¥   scrapy startproject  é¡¹ç›®åç§°
```

```
2.é¡¹ç›®ç»„æˆï¼š
          spiders 
              __init__.py
              è‡ªå®šä¹‰çš„çˆ¬è™«æ–‡ä»¶.py       ---ã€‹ç”±æˆ‘ä»¬è‡ªå·±åˆ›å»ºï¼Œæ˜¯å®ç°çˆ¬è™«æ ¸å¿ƒåŠŸèƒ½çš„æ–‡ä»¶
          __init__.py                  
          items.py                     ---ã€‹å®šä¹‰æ•°æ®ç»“æ„çš„åœ°æ–¹ï¼Œæ˜¯ä¸€ä¸ªç»§æ‰¿è‡ªscrapy.Itemçš„ç±»
          middlewares.py               ---ã€‹ä¸­é—´ä»¶   ä»£ç†
          pipelines.py				  ---ã€‹ç®¡é“æ–‡ä»¶ï¼Œé‡Œé¢åªæœ‰ä¸€ä¸ªç±»ï¼Œç”¨äºå¤„ç†ä¸‹è½½æ•°æ®çš„åç»­å¤„ç†
										é»˜è®¤æ˜¯300ä¼˜å…ˆçº§ï¼Œè¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼ˆ1-1000ï¼‰
          settings.py				  ---ã€‹é…ç½®æ–‡ä»¶  æ¯”å¦‚ï¼šæ˜¯å¦éµå®ˆrobotsåè®®ï¼ŒUser-Agentå®šä¹‰ç­‰
```

```
3.åˆ›å»ºçˆ¬è™«æ–‡ä»¶ï¼š
			ï¼ˆ1ï¼‰è·³è½¬åˆ°spidersæ–‡ä»¶å¤¹   cd ç›®å½•åå­—/ç›®å½•åå­—/spiders
			ï¼ˆ2ï¼‰scrapy genspider çˆ¬è™«åå­— ç½‘é¡µçš„åŸŸå
  çˆ¬è™«æ–‡ä»¶çš„åŸºæœ¬ç»„æˆï¼š
  			 ç»§æ‰¿scrapy.Spiderç±»
                                name = 'qiubai'       ---ã€‹  è¿è¡Œçˆ¬è™«æ–‡ä»¶æ—¶ä½¿ç”¨çš„åå­—
                                allowed_domains       ---ã€‹ çˆ¬è™«å…è®¸çš„åŸŸåï¼Œåœ¨çˆ¬å–çš„æ—¶å€™ï¼Œå¦‚æœä¸æ˜¯æ­¤åŸŸåä¹‹ä¸‹çš„urlï¼Œä¼šè¢«è¿‡æ»¤æ‰
                                start_urls			 ---ã€‹ å£°æ˜äº†çˆ¬è™«çš„èµ·å§‹åœ°å€ï¼Œå¯ä»¥å†™å¤šä¸ªurlï¼Œä¸€èˆ¬æ˜¯ä¸€ä¸ª
                                parse(self, response) ---ã€‹è§£ææ•°æ®çš„å›è°ƒå‡½æ•°
                                        response.text
                                        response.body ---ã€‹å“åº”çš„æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶
                                        response.xpath()
                                extract()             ---ã€‹æå–çš„æ˜¯selectorå¯¹è±¡çš„æ˜¯data
                                extract_first()       ---ã€‹æå–çš„æ˜¯selectoråˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªæ•°æ®
```

```
4.è¿è¡Œçˆ¬è™«æ–‡ä»¶ï¼š
			scrapy crawl çˆ¬è™«åç§°
			æ³¨æ„ï¼šåº”åœ¨spidersæ–‡ä»¶å¤¹å†…æ‰§è¡Œ
```

æ‰©å±•ï¼šå¯¼å‡ºæ–‡ä»¶

```
-o name.json
-o name.xml
-o name.csv
```

### scrapyæ¶æ„ç»„æˆ

```
        ï¼ˆ1ï¼‰å¼•æ“           		---ã€‹è‡ªåŠ¨è¿è¡Œï¼Œæ— éœ€å…³æ³¨ï¼Œä¼šè‡ªåŠ¨ç»„ç»‡æ‰€æœ‰çš„è¯·æ±‚å¯¹è±¡ï¼Œåˆ†å‘ç»™ä¸‹è½½å™¨
        ï¼ˆ2ï¼‰ä¸‹è½½å™¨				   ---ã€‹ä»å¼•æ“å¤„è·å–åˆ°è¯·æ±‚å¯¹è±¡åï¼Œè¯·æ±‚æ•°æ®
        ï¼ˆ3ï¼‰spiders				 ---ã€‹Spiderç±»å®šä¹‰äº†å¦‚ä½•çˆ¬å–æŸä¸ª(æˆ–æŸäº›)ç½‘ç«™ã€‚åŒ…æ‹¬äº†çˆ¬å–çš„åŠ¨ä½œ(ä¾‹å¦‚:æ˜¯å¦è·Ÿè¿›é“¾æ¥)ä»¥åŠå¦‚ä½•ä»ç½‘é¡µçš„å†…å®¹ä¸­æå–ç»“æ„åŒ–æ•°æ®(çˆ¬å–item)ã€‚ æ¢å¥è¯è¯´ï¼ŒSpiderå°±æ˜¯æ‚¨å®šä¹‰çˆ¬å–çš„åŠ¨ä½œåŠåˆ†ææŸä¸ªç½‘é¡µ(æˆ–è€…æ˜¯æœ‰äº›ç½‘é¡µ)çš„åœ°æ–¹ã€‚
        ï¼ˆ4ï¼‰è°ƒåº¦å™¨				   ---ã€‹æœ‰è‡ªå·±çš„è°ƒåº¦è§„åˆ™ï¼Œæ— éœ€å…³æ³¨
        ï¼ˆ5ï¼‰ç®¡é“ï¼ˆItem pipelineï¼‰   ---ã€‹æœ€ç»ˆå¤„ç†æ•°æ®çš„ç®¡é“ï¼Œä¼šé¢„ç•™æ¥å£ä¾›æˆ‘ä»¬å¤„ç†æ•°æ®
å½“Itemåœ¨Spiderä¸­è¢«æ”¶é›†ä¹‹åï¼Œå®ƒå°†ä¼šè¢«ä¼ é€’åˆ°Item Pipelineï¼Œä¸€äº›ç»„ä»¶ä¼šæŒ‰ç…§ä¸€å®šçš„é¡ºåºæ‰§è¡Œå¯¹Itemçš„å¤„ç†ã€‚
æ¯ä¸ªitem pipelineç»„ä»¶(æœ‰æ—¶ç§°ä¹‹ä¸ºâ€œItem Pipelineâ€)æ˜¯å®ç°äº†ç®€å•æ–¹æ³•çš„Pythonç±»ã€‚ä»–ä»¬æ¥æ”¶åˆ°Itemå¹¶é€šè¿‡å®ƒæ‰§è¡Œä¸€äº›è¡Œä¸ºï¼ŒåŒæ—¶ä¹Ÿå†³å®šæ­¤Itemæ˜¯å¦ç»§ç»­é€šè¿‡pipelineï¼Œæˆ–æ˜¯è¢«ä¸¢å¼ƒè€Œä¸å†è¿›è¡Œå¤„ç†ã€‚
          ä»¥ä¸‹æ˜¯item pipelineçš„ä¸€äº›å…¸å‹åº”ç”¨ï¼š
          1. æ¸…ç†HTMLæ•°æ®
          2. éªŒè¯çˆ¬å–çš„æ•°æ®(æ£€æŸ¥itemåŒ…å«æŸäº›å­—æ®µ)
          3. æŸ¥é‡(å¹¶ä¸¢å¼ƒ)
          4. å°†çˆ¬å–ç»“æœä¿å­˜åˆ°æ•°æ®åº“ä¸­
```

### scrapyå·¥ä½œåŸç†

![scrapyåŸç†](D:\markdown\data\scrapyåŸç†.png)

### scrapy shell

```
1.ä»€ä¹ˆæ˜¯scrapy shellï¼Ÿ
	Scrapyç»ˆç«¯ï¼Œæ˜¯ä¸€ä¸ªäº¤äº’ç»ˆç«¯ï¼Œä¾›æ‚¨åœ¨æœªå¯åŠ¨spiderçš„æƒ…å†µä¸‹å°è¯•åŠè°ƒè¯•æ‚¨çš„çˆ¬å–ä»£ç ã€‚ å…¶æœ¬æ„æ˜¯ç”¨æ¥æµ‹è¯•æå–æ•°æ®çš„ä»£ç ï¼Œä¸è¿‡æ‚¨å¯ä»¥å°†å…¶ä½œä¸ºæ­£å¸¸çš„Pythonç»ˆç«¯ï¼Œåœ¨ä¸Šé¢æµ‹è¯•ä»»ä½•çš„Pythonä»£ç ã€‚
è¯¥ç»ˆç«¯æ˜¯ç”¨æ¥æµ‹è¯•XPathæˆ–CSSè¡¨è¾¾å¼ï¼ŒæŸ¥çœ‹ä»–ä»¬çš„å·¥ä½œæ–¹å¼åŠä»çˆ¬å–çš„ç½‘é¡µä¸­æå–çš„æ•°æ®ã€‚ åœ¨ç¼–å†™æ‚¨çš„spideræ—¶ï¼Œè¯¥ç»ˆç«¯æä¾›äº†äº¤äº’æ€§æµ‹è¯•æ‚¨çš„è¡¨è¾¾å¼ä»£ç çš„åŠŸèƒ½ï¼Œå…å»äº†æ¯æ¬¡ä¿®æ”¹åè¿è¡Œspiderçš„éº»çƒ¦ã€‚
ä¸€æ—¦ç†Ÿæ‚‰äº†Scrapyç»ˆç«¯åï¼Œæ‚¨ä¼šå‘ç°å…¶åœ¨å¼€å‘å’Œè°ƒè¯•spideræ—¶å‘æŒ¥çš„å·¨å¤§ä½œç”¨ã€‚
```

```
2.å®‰è£…ipython
	å®‰è£…ï¼špip install ipython
	ç®€ä»‹ï¼šå¦‚æœæ‚¨å®‰è£…äº† IPython ï¼ŒScrapyç»ˆç«¯å°†ä½¿ç”¨ IPython (æ›¿ä»£æ ‡å‡†Pythonç»ˆç«¯)ã€‚ IPython ç»ˆç«¯ä¸å…¶ä»–ç›¸æ¯”æ›´ä¸ºå¼ºå¤§ï¼Œæä¾›æ™ºèƒ½çš„è‡ªåŠ¨è¡¥å…¨ï¼Œé«˜äº®è¾“å‡ºï¼ŒåŠå…¶ä»–ç‰¹æ€§ã€‚
```

```
3.åº”ç”¨ï¼šï¼ˆ1ï¼‰scrapy shell www.baidu.com
       ï¼ˆ2ï¼‰scrapy shell http://www.baidu.com
        (3) scrapy shell "http://www.baidu.com"
        (4) scrapy shell "www.baidu.com"

è¯­æ³•ï¼š
	ï¼ˆ1ï¼‰responseå¯¹è±¡ï¼š
                  response.body
                  response.text
                  response.url
                  response.status
    ï¼ˆ2ï¼‰responseçš„è§£æï¼š
                  response.xpath()
                  			 ä½¿ç”¨xpathè·¯å¾„æŸ¥è¯¢ç‰¹å®šå…ƒç´ ï¼Œè¿”å›ä¸€ä¸ªselectoråˆ—è¡¨å¯¹è±¡
                  response.css()
                  			 ä½¿ç”¨css_selectoræŸ¥è¯¢å…ƒç´ ï¼Œè¿”å›ä¸€ä¸ªselectoråˆ—è¡¨å¯¹è±¡
                              è·å–å†…å®¹ ï¼šresponse.css('#su::text').extract_first()
                              è·å–å±æ€§ ï¼šresponse.css('#su::attr(â€œvalueâ€)').extract_first()
    ï¼ˆ3ï¼‰selectorå¯¹è±¡ï¼ˆé€šè¿‡xpathæ–¹æ³•è°ƒç”¨è¿”å›çš„æ˜¯seletoråˆ—è¡¨ï¼‰
                  extract()
                  			 ä½¿ç”¨xpathè¯·æ±‚åˆ°çš„å¯¹è±¡æ˜¯ä¸€ä¸ªselectorå¯¹è±¡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä½¿ç”¨extract()æ–¹æ³•æ‹†åŒ…ï¼Œè½¬æ¢ä¸ºunicodeå­—ç¬¦ä¸²
                  extract_first()
                  			 è¿”å›ç¬¬ä¸€ä¸ªè§£æåˆ°çš„å€¼ï¼Œå¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œæ­¤ç§æ–¹æ³•ä¹Ÿä¸ä¼šæŠ¥é”™ï¼Œä¼šè¿”å›ä¸€ä¸ªç©ºå€¼
                  xpath()
                  css()
                  			 æ³¨æ„ï¼šæ¯ä¸€ä¸ªselectorå¯¹è±¡å¯ä»¥å†æ¬¡çš„å»ä½¿ç”¨xpathæˆ–è€…cssæ–¹æ³•

    ï¼ˆ4ï¼‰itemå¯¹è±¡
                  dict(itemobj)
                  			 å¯ä»¥ä½¿ç”¨dictæ–¹æ³•ç›´æ¥å°†itemå¯¹è±¡è½¬æ¢æˆå­—å…¸å¯¹è±¡
                  Item(dicobj)
                  			 å¯ä»¥ä½¿ç”¨å­—å…¸å¯¹è±¡åˆ›å»ºä¸€ä¸ªItemå¯¹è±¡
```

### yield

```
1. å¸¦æœ‰ yield çš„å‡½æ•°ä¸å†æ˜¯ä¸€ä¸ªæ™®é€šå‡½æ•°ï¼Œè€Œæ˜¯ä¸€ä¸ªç”Ÿæˆå™¨generatorï¼Œå¯ç”¨äºè¿­ä»£
2. yield æ˜¯ä¸€ä¸ªç±»ä¼¼ return çš„å…³é”®å­—ï¼Œè¿­ä»£ä¸€æ¬¡é‡åˆ°yieldæ—¶å°±è¿”å›yieldåé¢(å³è¾¹)çš„å€¼ã€‚é‡ç‚¹æ˜¯ï¼šä¸‹ä¸€æ¬¡è¿­ä»£æ—¶ï¼Œä»ä¸Šä¸€æ¬¡è¿­ä»£é‡åˆ°çš„yieldåé¢çš„ä»£ç (ä¸‹ä¸€è¡Œ)å¼€å§‹æ‰§è¡Œ
3. ç®€è¦ç†è§£ï¼šyieldå°±æ˜¯ return è¿”å›ä¸€ä¸ªå€¼ï¼Œå¹¶ä¸”è®°ä½è¿™ä¸ªè¿”å›çš„ä½ç½®ï¼Œä¸‹æ¬¡è¿­ä»£å°±ä»è¿™ä¸ªä½ç½®å(ä¸‹ä¸€è¡Œ)å¼€å§‹

name_list = [x for x in range(10)]
def createGenorator():
	items = []
	for i in name_list:
		print('ç¬¬{}æ¬¡è°ƒç”¨'.format(i))
		items.append(i)
	return items
def testFunc1():
	generator = createGenorator2()
	for a in generator:
		print('ä½¿ç”¨ç¬¬{}æ¬¡'.format(a))

def createGenorator2():
	for i in name_list:
		print('ç¬¬{}æ¬¡è°ƒç”¨'.format(i))
		yield i
print(testFunc1())
```

### pymysqlçš„ä½¿ç”¨æ­¥éª¤ï¼ˆsqlalchemyï¼‰

```
1.pip install pymysql
2.pymysql.connect(host,port,user,password,db,charset)
3.conn.cursor()
4.cursor.execute()

pip install sqlalchemy
import sqlalchemy
conn = sqlalchemy.create_engine('mysql+pymsql://user:password@host:port/database?charset')
charsetå¯ä»¥çœç•¥
```

### CrawlSpider

```
1.ç»§æ‰¿è‡ªscrapy.Spider
2.ç‹¬é—¨ç§˜ç¬ˆ
	CrawlSpiderå¯ä»¥å®šä¹‰è§„åˆ™ï¼Œå†è§£æhtmlå†…å®¹çš„æ—¶å€™ï¼Œå¯ä»¥æ ¹æ®é“¾æ¥è§„åˆ™æå–å‡ºæŒ‡å®šçš„é“¾æ¥ï¼Œç„¶åå†å‘è¿™äº›é“¾æ¥å‘é€è¯·æ±‚
	æ‰€ä»¥ï¼Œå¦‚æœæœ‰éœ€è¦è·Ÿè¿›é“¾æ¥çš„éœ€æ±‚ï¼Œæ„æ€å°±æ˜¯çˆ¬å–äº†ç½‘é¡µä¹‹åï¼Œéœ€è¦æå–é“¾æ¥å†æ¬¡çˆ¬å–ï¼Œä½¿ç”¨CrawlSpideræ˜¯éå¸¸åˆé€‚çš„
```

```
3.æå–é“¾æ¥
	é“¾æ¥æå–å™¨ï¼Œåœ¨è¿™é‡Œå°±å¯ä»¥å†™è§„åˆ™æå–æŒ‡å®šé“¾æ¥
scrapy.linkextractors.LinkExtractor(
	 allow = (),           # æ­£åˆ™è¡¨è¾¾å¼  æå–ç¬¦åˆæ­£åˆ™çš„é“¾æ¥
	 deny = (),            # (ä¸ç”¨)æ­£åˆ™è¡¨è¾¾å¼  ä¸æå–ç¬¦åˆæ­£åˆ™çš„é“¾æ¥
	 allow_domains = (),   # ï¼ˆä¸ç”¨ï¼‰å…è®¸çš„åŸŸå
	 deny_domains = (),    # ï¼ˆä¸ç”¨ï¼‰ä¸å…è®¸çš„åŸŸå
	 restrict_xpaths = (), # xpathï¼Œæå–ç¬¦åˆxpathè§„åˆ™çš„é“¾æ¥
	 restrict_css = ()     # æå–ç¬¦åˆé€‰æ‹©å™¨è§„åˆ™çš„é“¾æ¥)
4.æ¨¡æ‹Ÿä½¿ç”¨
		æ­£åˆ™ç”¨æ³•ï¼šlinks1 = LinkExtractor(allow=r'list_23_\d+\.html')
		xpathç”¨æ³•ï¼šlinks2 = LinkExtractor(restrict_xpaths=r'//div[@class="x"]')
		cssç”¨æ³•ï¼šlinks3 = LinkExtractor(restrict_css='.x')
5.æå–è¿æ¥
		link.extract_links(response)
```

```
6.æ³¨æ„äº‹é¡¹
	ã€æ³¨1ã€‘callbackåªèƒ½å†™å‡½æ•°åå­—ç¬¦ä¸², callback='parse_item'
	ã€æ³¨2ã€‘åœ¨åŸºæœ¬çš„spiderä¸­ï¼Œå¦‚æœé‡æ–°å‘é€è¯·æ±‚ï¼Œé‚£é‡Œçš„callbackå†™çš„æ˜¯   callback=self.parse_item ã€æ³¨--ç¨åçœ‹ã€‘follow=true æ˜¯å¦è·Ÿè¿› å°±æ˜¯æŒ‰ç…§æå–è¿æ¥è§„åˆ™è¿›è¡Œæå–
```

### CrawlSpideræ¡ˆä¾‹

```
1.åˆ›å»ºé¡¹ç›®ï¼šscrapy startproject dushuproject
2.è·³è½¬åˆ°spidersè·¯å¾„  cd\dushuproject\dushuproject\spiders
3.åˆ›å»ºçˆ¬è™«ç±»ï¼šscrapy genspider -t crawl read www.dushu.com
4.items
5.spiders
6.settings
7.pipelines
		æ•°æ®ä¿å­˜åˆ°æœ¬åœ°
		æ•°æ®ä¿å­˜åˆ°mysqlæ•°æ®åº“
```

### æ•°æ®å…¥åº“

```
ï¼ˆ1ï¼‰settingsé…ç½®å‚æ•°ï¼š
			 DB_HOST = '192.168.231.128'
              DB_PORT = 3306
              DB_USER = 'root'
              DB_PASSWORD = '1234'
              DB_NAME = 'test'
              DB_CHARSET = 'utf8'
ï¼ˆ2ï¼‰ç®¡é“é…ç½®
              from scrapy.utils.project import get_project_settings
              import pymysql
              class MysqlPipeline(object):
                  """docstring for MysqlPipeline"""
                  def __init__(self):
                      settings = get_project_settings()
                      self.host = settings['DB_HOST']
                      self.port = settings['DB_PORT']
                      self.user = settings['DB_USER']
                      self.pwd = settings['DB_PWD']
                      self.name = settings['DB_NAME']
                      self.charset = settings['DB_CHARSET']

                      self.connect()

                 def connect(self):
                      self.conn = pymysql.connect(host=self.host,
                                                 port=self.port,
                                                 user=self.user,
                                                 password=self.pwd,
                                                 db=self.name,
                                                 charset=self.charset)
                      self.cursor = self.conn.cursor()

                def close_spider(self, spider):
                    self.conn.close()
                    self.cursor.close()

                def process_item(self, item, spider):
                    sql = 'insert into book(image_url, book_name, author, info) values("%s", "%s", "%s", "%s")' % (item['image_url'], item['book_name'], item['author'], item['info'])
                    
                    sql = 'insert into book(image_url,book_name,author,info) values
 ("{}","{}","{}","{}")'.format(item['image_url'], item['book_name'], item['author'], item['info'])
                    # æ‰§è¡Œsqlè¯­å¥
                    self.cursor.execute(sql)
                    self.conn.commit()
                    return item
```

### æ—¥å¿—ä¿¡æ¯å’Œæ—¥å¿—ç­‰çº§

```
ï¼ˆ1ï¼‰æ—¥å¿—çº§åˆ«ï¼š
            CRITICALï¼šä¸¥é‡é”™è¯¯
            ERRORï¼šä¸€èˆ¬é”™è¯¯
            WARNINGï¼šè­¦å‘Š
            INFO: ä¸€èˆ¬ä¿¡æ¯
            DEBUGï¼šè°ƒè¯•ä¿¡æ¯
            
            é»˜è®¤çš„æ—¥å¿—ç­‰çº§æ˜¯DEBUG
            åªè¦å‡ºç°äº†DEBUGæˆ–è€…DEBUGä»¥ä¸Šç­‰çº§çš„æ—¥å¿—
            é‚£ä¹ˆè¿™äº›æ—¥å¿—å°†ä¼šæ‰“å°
ï¼ˆ2ï¼‰settings.pyæ–‡ä»¶è®¾ç½®ï¼š
		   é»˜è®¤çš„çº§åˆ«ä¸ºDEBUGï¼Œä¼šæ˜¾ç¤ºä¸Šé¢æ‰€æœ‰çš„ä¿¡æ¯
            åœ¨é…ç½®æ–‡ä»¶ä¸­  settings.py
            LOG_FILE  : ï¼ˆæ–‡ä»¶åï¼‰å°†å±å¹•æ˜¾ç¤ºçš„ä¿¡æ¯å…¨éƒ¨è®°å½•åˆ°æ–‡ä»¶ä¸­ï¼Œå±å¹•ä¸å†æ˜¾ç¤ºï¼Œæ³¨æ„æ–‡ä»¶åç¼€ä¸€å®šæ˜¯.log
            LOG_LEVEL : ï¼ˆä¸Šé¢çš„æ—¥å¿—çº§åˆ«ï¼‰è®¾ç½®æ—¥å¿—æ˜¾ç¤ºçš„ç­‰çº§ï¼Œå°±æ˜¯æ˜¾ç¤ºå“ªäº›ï¼Œä¸æ˜¾ç¤ºå“ªäº›
```

### Requestå’Œresponseæ€»ç»“

```
Request æ˜¯ä¸€ä¸ªç±»
	getè¯·æ±‚
		scrapy.Request(url=url, callback=self.parse_item, meta={'item': item}, headers=headers)
                url: è¦è¯·æ±‚çš„åœ°å€
                callbackï¼šå“åº”æˆåŠŸä¹‹åçš„å›è°ƒå‡½æ•°
                meta: å‚æ•°ä¼ é€’  æ¥æ”¶çš„è¯­æ³•ï¼šitem = response.meta['item']
                headers: å®šåˆ¶å¤´ä¿¡æ¯ï¼Œä¸€èˆ¬ä¸ç”¨      parse_itemæ–¹æ³•ä¸­çš„responseå‚æ•°å°±æ˜¯urlæ‰§è¡Œä¹‹åçš„è¯·æ±‚ç»“æœ
```

```
response æ˜¯ä¸€ä¸ªå¯¹è±¡ å‡½æ•°çš„ç¬¬äºŒä¸ªå‚æ•°
               	response.text: å­—ç¬¦ä¸²æ ¼å¼çš„æ–‡æœ¬
                response.body: äºŒè¿›åˆ¶æ ¼å¼çš„æ–‡æœ¬
                response.url: å½“å‰å“åº”çš„urlåœ°å€
                response.status: çŠ¶æ€ç 
                response.xpath(): ç­›é€‰ä½ æƒ³è¦çš„å†…å®¹
                response.css(): ç­›é€‰ä½ æƒ³è¦çš„å†…å®¹
```

### scrapyçš„postè¯·æ±‚

```
ï¼ˆ1ï¼‰é‡å†™start_requestsæ–¹æ³•ï¼š
		def start_requests(self)
 (2) start_requestsçš„è¿”å›å€¼ï¼š
 	 scrapy.FormRequest(url=url, headers=headers, callback=self.parse_item, formdata=data)
            url: è¦å‘é€çš„poståœ°å€
            headersï¼šå¯ä»¥å®šåˆ¶å¤´ä¿¡æ¯
            callback: å›è°ƒå‡½æ•°   
            formdata: postæ‰€æºå¸¦çš„æ•°æ®ï¼Œè¿™æ˜¯ä¸€ä¸ªå­—å…¸
```

### ä»£ç†ï¼ˆé€šè¿‡ä¸‹è½½ä¸­é—´ä»¶æ¥è¿›è¡Œæ·»åŠ ï¼‰

```
	ï¼ˆ1ï¼‰åˆ°settings.pyä¸­ï¼Œæ‰“å¼€ä¸€ä¸ªé€‰é¡¹
		DOWNLOADER_MIDDLEWARES = {
		   'postproject.middlewares.Proxy': 543,
		}
	ï¼ˆ2ï¼‰åˆ°middlewares.pyä¸­å†™ä»£ç 
		def process_request(self, request, spider):
	        request.meta['proxy'] = 'https://113.68.202.10:9999'
	        return None
```

### cookieç™»é™†

```
æ¡ˆä¾‹ï¼šå¾®åšç½‘ç™»é™†
ä»£ç ï¼š
import scrapy
class WbSpider(scrapy.Spider):
    name = 'wb'
    allowed_domains = ['weibo.cn']
    
    def start_requests(self):
        url = 'https://passport.weibo.cn/sso/login'
        headers={
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',
            # 'accept-encoding': 'gzip, deflate, br',
            'accept-language': 'zh-CN,zh;q=0.9',
            'cache-control': 'max-age=0',
            'cookie': 'SCF=Ahi2Sm3XHpcYIJvIsbJd8AnqkyO8t5RFmHXn8yHeTOMYgumvEqFGsgNbZbD6BmzlV7GA-B8sNWcbTcHeVmF3eNc.; _T_WM=32e4d519b787dd16fa96e2224b27a576; SUB=_2A25x8xaODeRhGeBK7lMV-S_JwzqIHXVTH7rGrDV6PUJbkdAKLVHlkW1NR6e0UBazrACpD1Cukh54U8WxKn23yIXs; SUHB=08JLxm5YJpiTJ1; SSOLoginState=1559717598',
            'referer': 'https://weibo.cn/',
            'upgrade-insecure-requests': '1',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
        }
        data = {
            'username': '18642820892',
            'password': 'lijing1150501',
            'savestate': '1',
            'r': 'https://weibo.cn/',
            'ec': '0',
            'pagerefer': 'https://weibo.cn/pub/',
            'entry': 'mweibo',
            'wentry': '',
            'loginfrom': '',
            'client_id': '',
            'code': '',
            'qq': '',
            'mainpageflag': '1',
            'hff': '',
            'hfp': '',
        }
        yield scrapy.FormRequest(url=url,callback=self.parse_item,formdata=data,headers=headers)

    def parse_item(self,response):
        url = 'https://weibo.cn/6451491586/info'
        headers = {
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',
            # 'accept-encoding': 'gzip, deflate, br',
            'accept-language': 'zh-CN,zh;q=0.9',
            'cache-control': 'max-age=0',
            'cookie': 'SCF=Ahi2Sm3XHpcYIJvIsbJd8AnqkyO8t5RFmHXn8yHeTOMYgumvEqFGsgNbZbD6BmzlV7GA-B8sNWcbTcHeVmF3eNc.; _T_WM=32e4d519b787dd16fa96e2224b27a576; SUB=_2A25x8xllDeRhGeBK7lMV-S_JwzqIHXVTH6ctrDV6PUJbkdAKLRjYkW1NR6e0UHabYnrx2IRT40S735NJ0JgW9T6S; SUHB=0LciE0Eb6CLyKh; SSOLoginState=1559718197',
            'referer': 'https://weibo.cn/',
            'upgrade-insecure-requests': '1',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
        }
        yield scrapy.Request(url=url,callback=self.parse_item1,headers=headers)

    def parse_item1(self,response):
        content = response.text
        with open('weibo.html','w',encoding='utf-8')as fp:
            fp.write(content)
```

### åˆ†å¸ƒå¼çˆ¬è™«å‡†å¤‡å·¥ä½œ

1.linuxä¸‹Pythonç¯å¢ƒæ­å»ºã€scrapyå®‰è£…ã€‚

```
pip3 install scrapy
pip3 install scrapy_redis
```

2.rediså®‰è£…

```
windows
	ï¼ˆ1ï¼‰ä¸‹è½½msiå®‰è£…åŒ…ï¼Œå®‰è£…è¿‡ç¨‹éœ€è¦å°†æ·»åŠ ç¯å¢ƒå˜é‡ã€è¿‡æ»¤é˜²ç«å¢™é€‰ä¸­ï¼Œå†…å­˜ä½¿ç”¨é»˜è®¤100Må³å¯
	ï¼ˆ2ï¼‰å¯åŠ¨ï¼š
              cd C:\Program Files\Redis
                  è·³è½¬åˆ°redisçš„å®‰è£…ç›®å½•ä¸‹
              redis-server.exe redis.windows.conf
                  å¯åŠ¨redisæœåŠ¡
              redis-cli
          å‘ç”Ÿå¯åŠ¨é”™è¯¯ä¿¡æ¯
              creating server tcp listening socket 127.0.0.1:6379: bind No error
          è§£å†³åŠæ³•
                  1. redis-cli.exe
                  2. shutdown
                  3. exit
                  4. redis-server.exe redis.windows.conf
	ï¼ˆ3ï¼‰æœ¬åœ°è¿æ¥
		redis-cli
	ï¼ˆ4ï¼‰è¿œç¨‹è¿æ¥é™åˆ¶
		windowsä¸Šé¢çš„redisé…ç½®æ–‡ä»¶ä¸­æœ‰é™åˆ¶ï¼Œåªå…è®¸æœ¬æœºè¿æ¥ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æ¥ä¿®æ”¹ä¸€ä¸‹é…ç½®ï¼Œè®©è¿œç¨‹linuxè¿æ¥
		æ¥åˆ°redisçš„å®‰è£…è·¯å¾„   C:\Program Files\Redis
		redisçš„é…ç½®æ–‡ä»¶å°±æ˜¯   redis.windows.conf
		ä¿®æ”¹é…ç½®æ–‡ä»¶
              ç¬¬56è¡Œ   æ³¨é‡Šæ‰è¿™ä¸€è¡Œ   å‰é¢æ·»åŠ #å·
              ç¬¬75è¡Œ   protected-mode no
	ï¼ˆ5ï¼‰è¿œç¨‹è¿æ¥
				æŒ‡ä»¤ï¼š(é»˜è®¤ç«¯å£å·éƒ½æ˜¯6379ï¼Œå¯ä»¥ä¸åŠ )
                  redis-cli -h host -p port      
                  redis-cli -h 10.11.63.79
æ³¨ï¼šlinuxå®‰è£…è§tornadoâ€”redisæ–‡æ¡£
```

1. æ‰©å±•ï¼š1.redisæ•°æ®ç±»å‹ï¼ˆhttp://www.runoob.com/redis/redis-sorted-sets.htmlï¼‰

   â€‹	    2.rediså¸¸è§é—®é¢˜ï¼ˆhttps://blog.csdn.net/hjm4702192/article/details/80518856ï¼‰

2. åˆ†å¸ƒå¼å®‰è£…

   windowsï¼špip install scrapy_redis

   linuxï¼š       pip3 install scrapy_redis

### scrapyå’Œscrapy_redisåŒºåˆ«ï¼Ÿ

```
ï¼ˆ1ï¼‰scrapyæ˜¯ä¸€ä¸ªé€šç”¨çš„çˆ¬è™«æ¡†æ¶ï¼Œä½†æ˜¯è¿™ä¸ªæ¡†æ¶ä¸æ”¯æŒåˆ†å¸ƒå¼
ï¼ˆ2ï¼‰scrapy_rediså°±æ˜¯ä¸ºäº†å®ç°scrapyçš„åˆ†å¸ƒå¼è€Œè¯ç”Ÿçš„ï¼Œå®ƒæä¾›äº†ä¸€äº›åŸºäºredisçš„ç»„ä»¶
	ï¼ˆhttps://www.cnblogs.com/nick477931661/p/9135497.htmlï¼‰
```

### scrapy_rediså®˜æ–¹æ¡ˆä¾‹ä»‹ç»

ï¼ˆhttps://github.com/rmax/scrapy-redisï¼‰

```
1.å£°æ˜æ™®é€šçš„spiderç±»ç»§æ‰¿çš„æ˜¯scrapy.spider
2.dmoz æ™®é€šçš„crawlspiderç±»
                        è¿æ¥æå–çš„spider
3.myspider_redis ç»§æ‰¿çš„æ˜¯RedisSpider
            ï¼ˆ1ï¼‰æ™®é€šçš„spiderçš„åˆ†å¸ƒå¼
            ï¼ˆ2ï¼‰  def __init__(self, *args, **kwargs):
                         # Dynamically define the allowed domains list.
                         domain = kwargs.pop('domain', '')
                         self.allowed_domains = filter(None, domain.split(','))
                         super(MySpider, self).__init__(*args, **kwargs)
                         å®˜æ–¹æ–‡æ¡£æŒ‡å®šçš„initçš„æš‚æ—¶ä¸å¯ä»¥ä»£æ›¿allowed_domains
                         æ‰€ä»¥æˆ‘ä»¬è¿˜æ˜¯è¦ä½¿ç”¨allowd_domains
             (3)æŒ‡å®šredis_key
                eg:redis_key = 'mycrawler:start_urls'
4.mycrawler_redis ç»§æ‰¿çš„æ˜¯RedisCrawlSpider
             (1)è¿æ¥æå–çš„åˆ†å¸ƒå¼
             (2)def __init__(self, *args, **kwargs):
                      # Dynamically define the allowed domains list.
                      domain = kwargs.pop('domain', '')
                      self.allowed_domains = filter(None, domain.split(','))
                      super(MySpider, self).__init__(*args, **kwargs)
                      å®˜æ–¹æ–‡æ¡£æŒ‡å®šçš„initçš„æš‚æ—¶ä¸å¯ä»¥ä»£æ›¿allowed_domains
                      æ‰€ä»¥æˆ‘ä»¬è¿˜æ˜¯è¦ä½¿ç”¨allowd_domains
              (3)æŒ‡å®šredis_key
                 eg:redis_key = 'mycrawler:start_urls'
5.æ–°å¢ç»„ä»¶
from scrapy_redis.spiders import RedisSpider
from scrapy_redis.spiders import RedisCrawlSpider
è¿™ä¸¤ä¸ªå°±æ˜¯scrapy_redisæ–°å¢åŠ çš„ä¸¤ä¸ªç»„ä»¶ï¼Œéƒ½æ˜¯ç»§æ‰¿è‡ªå®˜æ–¹çš„Spiderã€CrawlSpider
```

```
DOWNLOAD_DELAY = 1
ã€æ³¨ã€‘åœ¨çˆ¬å–ç½‘ç«™çš„æ—¶å€™ï¼Œå°†è¿™ä¸ªé€‰é¡¹æ‰“å¼€ï¼Œç»™å¯¹æ–¹ä¸€æ¡æ´»è·¯
```

### å¤šå°ç”µè„‘éƒ¨ç½²åˆ†å¸ƒå¼

```
ç°åœ¨æœ‰4å°ç”µè„‘ï¼šwindows   centos  ubuntu   macos
windowsä¸Šé¢å®‰è£…çš„æ˜¯redisæœåŠ¡å™¨ï¼Œmasterç«¯
	centosã€ubuntuã€macoséƒ½ä»redisä¸Šé¢è·å–è¯·æ±‚ï¼Œæˆ–è€…å°†è¯·æ±‚æ·»åŠ åˆ°redisæœåŠ¡å™¨ä¸­ï¼Œ   slaveç«¯
	slaveç«¯é¦–å…ˆå¯åŠ¨ï¼Œé¡µé¢å°±ä¼šåœæ­¢åœ¨é‚£é‡Œç­‰å¾…æŒ‡ä»¤
	è¿™ä¸ªæ—¶å€™masteré€šè¿‡lpushå‘é˜Ÿåˆ—ä¸­æ·»åŠ ä¸€ä¸ªèµ·å§‹urlï¼Œå…¶ä¸­ä¸€ä¸ªslaveç«¯è·å–åˆ°è¿™ä¸ªurlå¼€å§‹çˆ¬å–ï¼Œè¿™ä¸ªslaveç«¯ä¼šå°†è§£æä¹‹åçš„å¾ˆå¤šurlå†æ¬¡çš„æ·»åŠ åˆ°redisæœåŠ¡ä¸­ï¼Œç„¶åredisæœåŠ¡å†ä»è¯·æ±‚é˜Ÿåˆ—ä¸­å‘æ¯ä¸ªslaveç«¯åˆ†å‘è¯·æ±‚ï¼Œæœ€ç»ˆç›´åˆ°æ‰€æœ‰è¯·æ±‚çˆ¬å–å®Œæ¯•ä¸ºæ­¢
```

```
åˆ†å¸ƒå¼settingsåŸºæœ¬é…ç½®ï¼š
             ï¼ˆ1ï¼‰ä½¿ç”¨çš„æ˜¯scrapy_redisçš„å»é‡ç±»
              	  DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
             ï¼ˆ2ï¼‰è°ƒåº¦å™¨ä½¿ç”¨æ˜¯scrapy_redisçš„è°ƒåº¦å™¨
              	  SCHEDULER = "scrapy_redis.scheduler.Scheduler"
             ï¼ˆ3ï¼‰çˆ¬å–çš„è¿‡ç¨‹ä¸­æ˜¯å¦å…è®¸æš‚åœ
                  SCHEDULER_PERSIST = True
             ï¼ˆ4ï¼‰é…ç½®å­˜å‚¨çš„redisæœåŠ¡å™¨
                  REDIS_HOST = '10.11.52.62'
                  REDIS_PORT = 6379
                  ITEM_PIPELINES = {
                     'scrapy_redis.pipelines.RedisPipeline': 400,
                  }
```

```
scrapy-redisæ‰§è¡Œä»£ç ï¼š
	ï¼ˆ1ï¼‰slaveç«¯æ‰§è¡Œscrapy runspider mycrawler_redis.py
	ï¼ˆ2ï¼‰masterç«¯å‘é˜Ÿåˆ—ä¸­æ·»åŠ èµ·å§‹url
		è¿™ä¸ªkeyå°±æ˜¯ä½ ä»£ç ä¸­å†™çš„  redis_key
		lpush fen:start_urls 'http://www.dytt8.net/html/gndy/dyzz/index.html'
```

### åˆ†å¸ƒå¼çˆ¬è™«åŸç†



```
å¦‚ä½•å»é‡ï¼Ÿ
è¿™é‡Œå€ŸåŠ©redisçš„é›†åˆï¼Œredisæä¾›é›†åˆæ•°æ®ç»“æ„ï¼Œåœ¨redisé›†åˆä¸­å­˜å‚¨æ¯ä¸ªrequestçš„æŒ‡çº¹
åœ¨å‘requesté˜Ÿåˆ—ä¸­åŠ å…¥Requestå‰å…ˆéªŒè¯è¿™ä¸ªRequestçš„æŒ‡çº¹æ˜¯å¦å·²ç»åŠ å…¥é›†åˆä¸­ã€‚å¦‚æœå·²ç»å­˜åœ¨åˆ™ä¸æ·»åŠ åˆ°requesté˜Ÿåˆ—ä¸­ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™å°†requeståŠ å…¥åˆ°é˜Ÿåˆ—å¹¶å°†æŒ‡çº¹åŠ å…¥é›†åˆ

å¦‚ä½•é˜²æ­¢ä¸­æ–­ï¼Ÿå¦‚æœæŸä¸ªslaveå› ä¸ºç‰¹æ®ŠåŸå› å®•æœºï¼Œå¦‚ä½•è§£å†³ï¼Ÿ
è¿™é‡Œæ˜¯åšäº†å¯åŠ¨åˆ¤æ–­ï¼Œåœ¨æ¯å°slaveçš„Scrapyå¯åŠ¨çš„æ—¶å€™éƒ½ä¼šåˆ¤æ–­å½“å‰redis requesté˜Ÿåˆ—æ˜¯å¦ä¸ºç©º
å¦‚æœä¸ä¸ºç©ºï¼Œåˆ™ä»é˜Ÿåˆ—ä¸­è·å–ä¸‹ä¸€ä¸ªrequestæ‰§è¡Œçˆ¬å–ã€‚å¦‚æœä¸ºç©ºåˆ™é‡æ–°å¼€å§‹çˆ¬å–ï¼Œç¬¬ä¸€å°ä¸›é›†æ‰§è¡Œçˆ¬å–å‘é˜Ÿåˆ—ä¸­æ·»åŠ request
```

ä½œä¸šï¼š1.é¢„ä¹ åˆ†å¸ƒå¼çˆ¬è™«

â€‹            2.æˆ¿å¤©ä¸‹   å…¨å›½æ‰€æœ‰çš„äºŒæ‰‹æˆ¿ï¼Œå’Œæ–°æˆ¿çš„æˆ¿æº â€‹

### æˆ¿å¤©ä¸‹æ•°æ®ä¸‹è½½

```
å®ç°ç›®æ ‡ï¼š
		(1).çˆ¬å–å…¨å›½æ‰€æœ‰åŸå¸‚çš„æ–°æˆ¿
         (2).çˆ¬å–å…¨å›½æ‰€æœ‰åŸå¸‚çš„äºŒæ‰‹æˆ¿
å®ç°æ•ˆæœï¼š
{
  "province": "è¾½å®",
  "city": "å¤§è¿",
  "name": " é€šå·å‡ä»·ä¸‰ä¸‡,èŠ±å›­æ´‹æˆ¿,ç°æˆ¿ç²¾è£…è¿‘åœ°é“,ç‹¬ç«‹å¤§æˆ¿æœ¬",
  "price": "234ä¸‡"
}{
  "province": "ç›´è¾–å¸‚",
  "city": "åŒ—äº¬",
  "name": " å­”é›€åŸåŸå±±è‘—åŒ—æ¬§è”æ’,è§‚æ™¯ä½³,ä¾å±±å»º,é€æ˜ä»·",
  "price": "280ä¸‡"
}
```

æ¡ˆä¾‹ï¼šå›½å®¶ç»Ÿè®¡å±€ï¼ˆhttp://www.stats.gov.cn/tjsj/tjbz/tjyqhdmhcxhfdm/2017/ï¼‰å…±è®¡68ä¸‡æ¡æ•°

#### 2.è¿›ç¨‹

```
è¿›ç¨‹ï¼šç›´è§‚ç‚¹è¯´ï¼Œä¿å­˜åœ¨ç¡¬ç›˜ä¸Šçš„ç¨‹åºè¿è¡Œä»¥åï¼Œä¼šåœ¨å†…å­˜ç©ºé—´é‡Œå½¢æˆä¸€ä¸ªç‹¬ç«‹çš„å†…å­˜ä½“ï¼Œè¿™ä¸ªå†…å­˜ä½“æœ‰è‡ªå·±ç‹¬ç«‹çš„åœ°å€ç©ºé—´ï¼Œæœ‰è‡ªå·±çš„å †ï¼Œä¸Šçº§æŒ‚é å•ä½æ˜¯æ“ä½œç³»ç»Ÿã€‚æ“ä½œç³»ç»Ÿä¼šä»¥è¿›ç¨‹ä¸ºå•ä½ï¼Œåˆ†é…ç³»ç»Ÿèµ„æºï¼ˆCPUæ—¶é—´ç‰‡ã€å†…å­˜ç­‰èµ„æºï¼‰ï¼Œè¿›ç¨‹æ˜¯èµ„æºåˆ†é…çš„æœ€å°å•ä½ã€
```

```
ç³»ç»Ÿç”±ä¸€ä¸ªä¸ªè¿›ç¨‹(ç¨‹åº)ç»„æˆ,ä¸€èˆ¬æƒ…å†µä¸‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬åŒºåŸŸï¼ˆtext regionï¼‰ã€æ•°æ®åŒºåŸŸï¼ˆdata regionï¼‰å’Œå †æ ˆï¼ˆstack regionï¼‰ã€‚
ï¼ˆ1ï¼‰æ–‡æœ¬åŒºåŸŸå­˜å‚¨å¤„ç†å™¨æ‰§è¡Œçš„ä»£ç 
ï¼ˆ2ï¼‰æ•°æ®åŒºåŸŸå­˜å‚¨å˜é‡å’Œè¿›ç¨‹æ‰§è¡ŒæœŸé—´ä½¿ç”¨çš„åŠ¨æ€åˆ†é…çš„å†…å­˜ï¼›
ï¼ˆ3ï¼‰å †æ ˆåŒºåŸŸå­˜å‚¨ç€æ´»åŠ¨è¿‡ç¨‹è°ƒç”¨çš„æŒ‡ä»¤å’Œæœ¬åœ°å˜é‡ã€‚
å› æ­¤è¿›ç¨‹çš„åˆ›å»ºå’Œé”€æ¯éƒ½æ˜¯ç›¸å¯¹äºç³»ç»Ÿèµ„æº,æ‰€ä»¥æ˜¯ä¸€ç§æ¯”è¾ƒæ˜‚è´µçš„æ“ä½œã€‚ 
è¿›ç¨‹æœ‰ä¸‰ä¸ªçŠ¶æ€:
              ç­‰å¾…æ€ï¼šç­‰å¾…æŸä¸ªäº‹ä»¶çš„å®Œæˆï¼›
              å°±ç»ªæ€ï¼šç­‰å¾…ç³»ç»Ÿåˆ†é…å¤„ç†å™¨ä»¥ä¾¿è¿è¡Œï¼›
              è¿è¡Œæ€ï¼šå æœ‰å¤„ç†å™¨æ­£åœ¨è¿è¡Œã€‚
è¿›ç¨‹æ˜¯æŠ¢å å¼çš„äº‰å¤ºCPUè¿è¡Œè‡ªèº«,è€ŒCPUå•æ ¸çš„æƒ…å†µä¸‹åŒä¸€æ—¶é—´åªèƒ½æ‰§è¡Œä¸€ä¸ªè¿›ç¨‹çš„ä»£ç ,ä½†æ˜¯å¤šè¿›ç¨‹çš„å®ç°åˆ™æ˜¯é€šè¿‡CPUé£å¿«çš„åˆ‡æ¢ä¸åŒè¿›ç¨‹,å› æ­¤ä½¿å¾—çœ‹ä¸Šå»å°±åƒæ˜¯å¤šä¸ªè¿›ç¨‹åœ¨åŒæ—¶è¿›è¡Œ.
```

#### 3.çº¿ç¨‹

```
çº¿ç¨‹ï¼Œæœ‰æ—¶è¢«ç§°ä¸ºè½»é‡çº§è¿›ç¨‹(Lightweight Processï¼ŒLWPï¼‰ï¼Œæ˜¯æ“ä½œç³»ç»Ÿè°ƒåº¦ï¼ˆCPUè°ƒåº¦ï¼‰æ‰§è¡Œçš„æœ€å°å•ä½ã€‚
ï¼ˆ1ï¼‰çº¿ç¨‹å±äºè¿›ç¨‹
ï¼ˆ2ï¼‰çº¿ç¨‹å…±äº«è¿›ç¨‹çš„å†…å­˜åœ°å€ç©ºé—´
ï¼ˆ3ï¼‰çº¿ç¨‹å‡ ä¹ä¸å æœ‰ç³»ç»Ÿèµ„æº
é€šä¿¡é—®é¢˜:   è¿›ç¨‹ç›¸å½“äºä¸€ä¸ªå®¹å™¨,è€Œçº¿ç¨‹è€Œæ˜¯è¿è¡Œåœ¨å®¹å™¨é‡Œé¢çš„,å› æ­¤å¯¹äºå®¹å™¨å†…çš„ä¸œè¥¿,çº¿ç¨‹æ˜¯å…±åŒäº«æœ‰çš„,å› æ­¤çº¿ç¨‹é—´çš„é€šä¿¡å¯ä»¥ç›´æ¥é€šè¿‡å…¨å±€å˜é‡è¿›è¡Œé€šä¿¡,ä½†æ˜¯ç”±æ­¤å¸¦æ¥çš„ä¾‹å¦‚å¤šä¸ªçº¿ç¨‹è¯»å†™åŒä¸€ä¸ªåœ°å€å˜é‡çš„æ—¶å€™åˆ™å°†å¸¦æ¥ä¸å¯é¢„æœŸçš„åæœ,å› æ­¤è¿™æ—¶å€™å¼•å…¥äº†å„ç§é”çš„ä½œç”¨,ä¾‹å¦‚äº’æ–¥é”ç­‰ã€‚
```

![çº¿ç¨‹](C:\Users\lijingAction\Desktop\SH-1905-çˆ¬è™«\day09\doc\çº¿ç¨‹.jpg)

```
ï¼ˆ1ï¼‰è¿›ç¨‹æ˜¯ç³»ç»Ÿåˆ†é…èµ„æºçš„æœ€å°å•ä½
ï¼ˆ2ï¼‰çº¿ç¨‹æ˜¯CPUè°ƒåº¦çš„æœ€å°å•ä½
ï¼ˆ3ï¼‰ç”±äºé»˜è®¤è¿›ç¨‹å†…åªæœ‰ä¸€ä¸ªçº¿ç¨‹,æ‰€ä»¥å¤šæ ¸CPUå¤„ç†å¤šè¿›ç¨‹å°±åƒæ˜¯ä¸€ä¸ªè¿›ç¨‹ä¸€ä¸ªæ ¸å¿ƒ
```

```
çº¿ç¨‹å’Œè¿›ç¨‹çš„ä¸Šä¸‹æ–‡åˆ‡æ¢æ­¥éª¤ï¼š
                è¿›ç¨‹åˆ‡æ¢åˆ†3æ­¥:
                ï¼ˆ1ï¼‰åˆ‡æ¢é¡µç›®å½•ä»¥ä½¿ç”¨æ–°çš„åœ°å€ç©ºé—´
                ï¼ˆ2ï¼‰åˆ‡æ¢å†…æ ¸æ ˆ
                ï¼ˆ3ï¼‰åˆ‡æ¢ç¡¬ä»¶ä¸Šä¸‹æ–‡
                    è€Œçº¿ç¨‹åˆ‡æ¢åªéœ€è¦ç¬¬2ã€3æ­¥,å› æ­¤è¿›ç¨‹çš„åˆ‡æ¢ä»£ä»·æ¯”è¾ƒå¤§
```

#### 4.åç¨‹

```
ï¼ˆ1ï¼‰åç¨‹æ˜¯å±äºçº¿ç¨‹çš„ã€‚åç¨‹ç¨‹åºæ˜¯åœ¨çº¿ç¨‹é‡Œé¢è·‘çš„ï¼Œå› æ­¤åç¨‹åˆç§°å¾®çº¿ç¨‹å’Œçº¤ç¨‹ç­‰
ï¼ˆ2ï¼‰åç¨‹æ²¡æœ‰çº¿ç¨‹çš„ä¸Šä¸‹æ–‡åˆ‡æ¢æ¶ˆè€—ã€‚åç¨‹çš„è°ƒåº¦åˆ‡æ¢æ˜¯ç”¨æˆ·(ç¨‹åºå‘˜)æ‰‹åŠ¨åˆ‡æ¢çš„,å› æ­¤æ›´åŠ çµæ´»,å› æ­¤åˆå«ç”¨æˆ·ç©ºé—´çº¿ç¨‹.
ï¼ˆ3ï¼‰åŸå­æ“ä½œæ€§ã€‚ç”±äºåç¨‹æ˜¯ç”¨æˆ·è°ƒåº¦çš„ï¼Œæ‰€ä»¥ä¸ä¼šå‡ºç°æ‰§è¡Œä¸€åŠçš„ä»£ç ç‰‡æ®µè¢«å¼ºåˆ¶ä¸­æ–­äº†ï¼Œå› æ­¤æ— éœ€åŸå­æ“ä½œé”ã€‚
```

åç¨‹çš„å®ç°

```
åç¨‹çš„å®ç°ï¼šè¿­ä»£å™¨å’Œç”Ÿæˆå™¨
            è¿­ä»£å™¨ï¼š å®ç°äº†è¿­ä»£æ¥å£çš„ç±»,æ¥å£å‡½æ•°ä¾‹å¦‚:current,key,next,rewind,validã€‚è¿­ä»£å™¨æœ€åŸºæœ¬çš„è§„å®šäº†å¯¹è±¡å¯ä»¥é€šè¿‡nextè¿”å›ä¸‹ä¸€ä¸ªå€¼ï¼Œè€Œä¸æ˜¯åƒæ•°ç»„ï¼Œåˆ—è¡¨ä¸€æ ·ä¸€æ¬¡æ€§è¿”å›ã€‚è¯­è¨€å®ç°ï¼šåœ¨Javaçš„foreachéå†è¿­ä»£å™¨å¯¹(æ•°ç»„)ï¼ŒPythonçš„foréå†è¿­ä»£å™¨å¯¹è±¡(tupleï¼Œlistï¼Œdict)ã€‚
            ç”Ÿæˆå™¨ï¼š ä½¿ç”¨ yield å…³é”®å­—çš„å‡½æ•°,å¯ä»¥å¤šæ¬¡è¿”å›å€¼ï¼Œç”Ÿæˆå™¨å®é™…ä¸Šä¹Ÿç®—æ˜¯å®ç°äº†è¿­ä»£å™¨æ¥å£(åè®®)ã€‚å³ç”Ÿæˆå™¨ä¹Ÿå¯é€šè¿‡nextè¿”å›ä¸‹ä¸€ä¸ªå€¼ã€‚
```

```
åç¨‹ä¸¾ä¾‹ï¼šåœ¨Pythonä¸­ï¼Œä½¿ç”¨äº†yieldçš„å‡½æ•°ä¸ºç”Ÿæˆå™¨å‡½æ•°ï¼Œå³å¯ä»¥å¤šæ¬¡è¿”å›å€¼ã€‚åˆ™ç”Ÿæˆå™¨å¯ä»¥æš‚åœä¸€ä¸‹ï¼Œè½¬è€Œæ‰§è¡Œå…¶ä»–ä»£ç ï¼Œå†å›æ¥ç»§ç»­æ‰§è¡Œå‡½æ•°å¾€ä¸‹çš„ä»£ç 
```

#### 5.æ€»ç»“ï¼šè¿›ç¨‹ã€çº¿ç¨‹å¯¹æ¯”

```
åŠŸèƒ½å¯¹æ¯”ï¼š
        è¿›ç¨‹ï¼Œèƒ½å¤Ÿå®Œæˆå¤šä»»åŠ¡ï¼Œæ¯”å¦‚ åœ¨ä¸€å°ç”µè„‘ä¸Šèƒ½å¤ŸåŒæ—¶è¿è¡Œå¤šä¸ªQQ
        çº¿ç¨‹ï¼Œèƒ½å¤Ÿå®Œæˆå¤šä»»åŠ¡ï¼Œæ¯”å¦‚ ä¸€ä¸ªQQä¸­çš„å¤šä¸ªèŠå¤©çª—å£
```

```
å®šä¹‰çš„ä¸åŒï¼š
        è¿›ç¨‹:ç³»ç»Ÿè¿›è¡Œèµ„æºåˆ†é…å’Œè°ƒåº¦çš„ä¸€ä¸ªç‹¬ç«‹å•ä½.
        çº¿ç¨‹:è¿›ç¨‹çš„ä¸€ä¸ªå®ä½“,æ˜¯CPUè°ƒåº¦å’Œåˆ†æ´¾çš„åŸºæœ¬å•ä½,å®ƒæ˜¯æ¯”è¿›ç¨‹æ›´å°çš„èƒ½ç‹¬ç«‹è¿è¡Œçš„åŸºæœ¬å•ä½.çº¿ç¨‹è‡ªå·±åŸºæœ¬ä¸Šä¸æ‹¥æœ‰ç³»ç»Ÿèµ„æº,åªæ‹¥æœ‰ä¸€ç‚¹åœ¨è¿è¡Œä¸­å¿…ä¸å¯å°‘çš„èµ„æº(å¦‚ç¨‹åºè®¡æ•°å™¨,ä¸€ç»„å¯„å­˜å™¨å’Œæ ˆ),ä½†æ˜¯å®ƒå¯ä¸åŒå±ä¸€ä¸ªè¿›ç¨‹çš„å…¶ä»–çš„çº¿ç¨‹å…±äº«è¿›ç¨‹æ‰€æ‹¥æœ‰çš„å…¨éƒ¨èµ„æº.
```

```
åŒºåˆ«:
        (1)ä¸€ä¸ªç¨‹åºè‡³å°‘æœ‰ä¸€ä¸ªè¿›ç¨‹,ä¸€ä¸ªè¿›ç¨‹è‡³å°‘æœ‰ä¸€ä¸ªçº¿ç¨‹.
        (2)çº¿ç¨‹çš„åˆ’åˆ†å°ºåº¦å°äºè¿›ç¨‹(èµ„æºæ¯”è¿›ç¨‹å°‘)ï¼Œä½¿å¾—å¤šçº¿ç¨‹ç¨‹åºçš„å¹¶å‘æ€§é«˜ã€‚
        (3)è¿›ç¨‹åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­æ‹¥æœ‰ç‹¬ç«‹çš„å†…å­˜å•å…ƒï¼Œè€Œå¤šä¸ªçº¿ç¨‹å…±äº«å†…å­˜ï¼Œä»è€Œæå¤§åœ°æé«˜äº†ç¨‹åºçš„è¿è¡Œæ•ˆç‡ 
        (4)çº¿ç¨‹ä¸èƒ½å¤Ÿç‹¬ç«‹æ‰§è¡Œï¼Œå¿…é¡»ä¾å­˜åœ¨è¿›ç¨‹ä¸­
        (5)å¯ä»¥å°†è¿›ç¨‹ç†è§£ä¸ºå·¥å‚ä¸­çš„ä¸€æ¡æµæ°´çº¿ï¼Œè€Œå…¶ä¸­çš„çº¿ç¨‹å°±æ˜¯è¿™ä¸ªæµæ°´çº¿ä¸Šçš„å·¥äºº 
```

```
ä¼˜ç¼ºç‚¹:
        çº¿ç¨‹å’Œè¿›ç¨‹åœ¨ä½¿ç”¨ä¸Šå„æœ‰ä¼˜ç¼ºç‚¹ï¼šçº¿ç¨‹æ‰§è¡Œå¼€é”€å°ï¼Œä½†ä¸åˆ©äºèµ„æºçš„ç®¡ç†å’Œä¿æŠ¤ï¼›è€Œè¿›ç¨‹æ­£ç›¸åã€‚
```

#### 6.æ€»ç»“ï¼šè¿›ç¨‹ã€çº¿ç¨‹ã€åç¨‹å¯¹æ¯”

```
ç®€ç­”æ¡ˆä¾‹ï¼š
        æœ‰ä¸€ä¸ªè€æ¿æƒ³è¦å¼€ä¸ªå·¥å‚è¿›è¡Œç”Ÿäº§æŸä»¶å•†å“ï¼ˆä¾‹å¦‚å‰ªå­ï¼‰
        ä»–éœ€è¦èŠ±ä¸€äº›è´¢åŠ›ç‰©åŠ›åˆ¶ä½œä¸€æ¡ç”Ÿäº§çº¿ï¼Œè¿™ä¸ªç”Ÿäº§çº¿ä¸Šæœ‰å¾ˆå¤šçš„å™¨ä»¶ä»¥åŠææ–™è¿™äº›æ‰€æœ‰çš„ ä¸ºäº†èƒ½å¤Ÿç”Ÿäº§å‰ªå­è€Œå‡†å¤‡çš„èµ„æºç§°ä¹‹ä¸ºï¼šè¿›ç¨‹
        åªæœ‰ç”Ÿäº§çº¿æ˜¯ä¸èƒ½å¤Ÿè¿›è¡Œç”Ÿäº§çš„ï¼Œæ‰€ä»¥è€æ¿çš„æ‰¾ä¸ªå·¥äººæ¥è¿›è¡Œç”Ÿäº§ï¼Œè¿™ä¸ªå·¥äººèƒ½å¤Ÿåˆ©ç”¨è¿™äº›ææ–™æœ€ç»ˆä¸€æ­¥æ­¥çš„å°†å‰ªå­åšå‡ºæ¥ï¼Œè¿™ä¸ªæ¥åšäº‹æƒ…çš„å·¥äººç§°ä¹‹ä¸ºï¼šçº¿ç¨‹
        è¿™ä¸ªè€æ¿ä¸ºäº†æé«˜ç”Ÿäº§ç‡ï¼Œæƒ³åˆ°3ç§åŠæ³•ï¼š
        ï¼ˆ1ï¼‰åœ¨è¿™æ¡ç”Ÿäº§çº¿ä¸Šå¤šæ‹›äº›å·¥äººï¼Œä¸€èµ·æ¥åšå‰ªå­ï¼Œè¿™æ ·æ•ˆç‡æ˜¯æˆå€å¢—é•¿ï¼Œå³å•è¿›ç¨‹ å¤šçº¿ç¨‹æ–¹å¼
        ï¼ˆ2ï¼‰è€æ¿å‘ç°è¿™æ¡ç”Ÿäº§çº¿ä¸Šçš„å·¥äººä¸æ˜¯è¶Šå¤šè¶Šå¥½ï¼Œå› ä¸ºä¸€æ¡ç”Ÿäº§çº¿çš„èµ„æºä»¥åŠææ–™æ¯•ç«Ÿæœ‰é™ï¼Œæ‰€ä»¥è€æ¿åˆèŠ±äº†äº›è´¢åŠ›ç‰©åŠ›è´­ç½®äº†å¦å¤–ä¸€æ¡ç”Ÿäº§çº¿ï¼Œç„¶åå†æ‹›äº›å·¥äººè¿™æ ·æ•ˆç‡åˆå†ä¸€æ­¥æé«˜äº†ï¼Œå³å¤šè¿›ç¨‹ å¤šçº¿ç¨‹æ–¹å¼
        ï¼ˆ3ï¼‰è€æ¿å‘ç°ï¼Œç°åœ¨å·²ç»æœ‰äº†å¾ˆå¤šæ¡ç”Ÿäº§çº¿ï¼Œå¹¶ä¸”æ¯æ¡ç”Ÿäº§çº¿ä¸Šå·²ç»æœ‰å¾ˆå¤šå·¥äººäº†ï¼ˆå³ç¨‹åºæ˜¯å¤šè¿›ç¨‹çš„ï¼Œæ¯ä¸ªè¿›ç¨‹ä¸­åˆæœ‰å¤šä¸ªçº¿ç¨‹ï¼‰ï¼Œä¸ºäº†å†æ¬¡æé«˜æ•ˆç‡ï¼Œè€æ¿æƒ³äº†ä¸ªæŸæ‹›ï¼Œè§„å®šï¼šå¦‚æœæŸä¸ªå‘˜å·¥åœ¨ä¸Šç­æ—¶ä¸´æ—¶æ²¡äº‹æˆ–è€…å†ç­‰å¾…æŸäº›æ¡ä»¶ï¼ˆæ¯”å¦‚ç­‰å¾…å¦ä¸€ä¸ªå·¥äººç”Ÿäº§å®Œè°‹é“å·¥åº ä¹‹åä»–æ‰èƒ½å†æ¬¡å·¥ä½œï¼‰ ï¼Œé‚£ä¹ˆè¿™ä¸ªå‘˜å·¥å°±åˆ©ç”¨è¿™ä¸ªæ—¶é—´å»åšå…¶å®ƒçš„äº‹æƒ…ï¼Œé‚£ä¹ˆä¹Ÿå°±æ˜¯è¯´ï¼šå¦‚æœä¸€ä¸ªçº¿ç¨‹ç­‰å¾…æŸäº›æ¡ä»¶ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨è¿™ä¸ªæ—¶é—´å»åšå…¶å®ƒäº‹æƒ…ï¼Œå…¶å®è¿™å°±æ˜¯ï¼šåç¨‹æ–¹å¼
```

```
ç®€å•æ€»ç»“ï¼š
        ï¼ˆ1ï¼‰è¿›ç¨‹æ˜¯èµ„æºåˆ†é…çš„å•ä½
        ï¼ˆ2ï¼‰çº¿ç¨‹æ˜¯æ“ä½œç³»ç»Ÿè°ƒåº¦çš„å•ä½
        ï¼ˆ3ï¼‰è¿›ç¨‹åˆ‡æ¢éœ€è¦çš„èµ„æºå¾ˆæœ€å¤§ï¼Œæ•ˆç‡å¾ˆä½
        ï¼ˆ4ï¼‰çº¿ç¨‹åˆ‡æ¢éœ€è¦çš„èµ„æºä¸€èˆ¬ï¼Œæ•ˆç‡ä¸€èˆ¬ï¼ˆå½“ç„¶äº†åœ¨ä¸è€ƒè™‘GILçš„æƒ…å†µä¸‹ï¼‰
        ï¼ˆ5ï¼‰åç¨‹åˆ‡æ¢ä»»åŠ¡èµ„æºå¾ˆå°ï¼Œæ•ˆç‡é«˜
        ï¼ˆ6ï¼‰å¤šè¿›ç¨‹ã€å¤šçº¿ç¨‹æ ¹æ®cpuæ ¸æ•°ä¸ä¸€æ ·å¯èƒ½æ˜¯å¹¶è¡Œçš„ï¼Œä½†æ˜¯åç¨‹æ˜¯åœ¨ä¸€ä¸ªçº¿ç¨‹ä¸­ æ‰€ä»¥æ˜¯å¹¶å‘
```

æ¡ˆä¾‹ï¼šå¤šçº¿ç¨‹çˆ¬å–ç³—äº‹ç™¾ç§‘
